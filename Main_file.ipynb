{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ALL installs "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ! pip install requests\n",
    "# ! pip install kafka-python\n",
    "# ! pip install pyspark\n",
    "# !pip  install -r requirements.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip freeze > requirements.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ALL imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "from allsecrets import *\n",
    "from coordinates import *\n",
    "from schemas import *\n",
    "import json,requests\n",
    "from json import dumps,loads \n",
    "from kafka import KafkaProducer, KafkaConsumer\n",
    "from datetime import datetime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession , functions as F\n",
    "from pyspark.sql.functions import udf,col,countDistinct,date_format,row_number\n",
    "from pyspark.sql.types import FloatType , StringType , IntegerType , StructType , StructField , TimestampType\n",
    "from pyspark.sql.window import Window \n",
    "\n",
    "spark = SparkSession.builder.appName('Weather')\\\n",
    "    .getOrCreate()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Json print function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def json_print(json_object):\n",
    "    text = json.dumps(json_object, sort_keys=True, indent=4) # sort_keys=True means sort the keys in alphabetical order\n",
    "    # indent 4 means 4 spaces for each indentation\n",
    "    print(text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1st Producer and consumer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Producer  to handle API response and send to Kafka topic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "Producer_API = KafkaProducer(bootstrap_servers=[\n",
    "                         'localhost:9092'], value_serializer=lambda x: dumps(x).encode('utf-8'))\n",
    "\n",
    "for lat, lon in lat_lon.items():\n",
    "    parameters = {'lat': lat, \"lon\": lon, \"appid\": open_weather_api}\n",
    "    weather_response = requests.get(\n",
    "        \"http://api.openweathermap.org/data/2.5/forecast\", params=parameters)\n",
    "    Producer_API.send('weather', value=weather_response.json())\n",
    "    Producer_API.flush()  # flush the data to the kafka broker ( topic) and  make sure data  is sent to the kafka broker and  not lost in the buffer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Consumer to read from Kafka and write to file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "# if consumer_dump.json file exists, delete it\n",
    "import os\n",
    "if os.path.exists('consumer_dump.json'):\n",
    "    os.remove('consumer_dump.json')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "Consumer_API = KafkaConsumer('weather', bootstrap_servers=['localhost:9092'], auto_offset_reset='earliest',\n",
    "                         enable_auto_commit=True, value_deserializer=lambda x: loads(x.decode('utf-8')))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn [41], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[39m# consume the data from the kafka broker and save to json file   %%%% stop it after some time   10-15 sec %%%%%\u001b[39;00m\n\u001b[0;32m----> 2\u001b[0m \u001b[39mfor\u001b[39;00m message \u001b[39min\u001b[39;00m Consumer_API:\n\u001b[1;32m      3\u001b[0m     message \u001b[39m=\u001b[39m message\u001b[39m.\u001b[39mvalue\n\u001b[1;32m      4\u001b[0m     \u001b[39mwith\u001b[39;00m \u001b[39mopen\u001b[39m(\u001b[39m'\u001b[39m\u001b[39mconsumer_dump.json\u001b[39m\u001b[39m'\u001b[39m, \u001b[39m'\u001b[39m\u001b[39ma\u001b[39m\u001b[39m'\u001b[39m) \u001b[39mas\u001b[39;00m f:\n",
      "File \u001b[0;32m~/All_Repos_Env/ENV_SPARK/lib/python3.10/site-packages/kafka/consumer/group.py:1193\u001b[0m, in \u001b[0;36mKafkaConsumer.__next__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1191\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mnext_v1()\n\u001b[1;32m   1192\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m-> 1193\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mnext_v2()\n",
      "File \u001b[0;32m~/All_Repos_Env/ENV_SPARK/lib/python3.10/site-packages/kafka/consumer/group.py:1201\u001b[0m, in \u001b[0;36mKafkaConsumer.next_v2\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1199\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_iterator \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_message_generator_v2()\n\u001b[1;32m   1200\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m-> 1201\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mnext\u001b[39;49m(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_iterator)\n\u001b[1;32m   1202\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mStopIteration\u001b[39;00m:\n\u001b[1;32m   1203\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_iterator \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/All_Repos_Env/ENV_SPARK/lib/python3.10/site-packages/kafka/consumer/group.py:1116\u001b[0m, in \u001b[0;36mKafkaConsumer._message_generator_v2\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1114\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m_message_generator_v2\u001b[39m(\u001b[39mself\u001b[39m):\n\u001b[1;32m   1115\u001b[0m     timeout_ms \u001b[39m=\u001b[39m \u001b[39m1000\u001b[39m \u001b[39m*\u001b[39m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_consumer_timeout \u001b[39m-\u001b[39m time\u001b[39m.\u001b[39mtime())\n\u001b[0;32m-> 1116\u001b[0m     record_map \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mpoll(timeout_ms\u001b[39m=\u001b[39;49mtimeout_ms, update_offsets\u001b[39m=\u001b[39;49m\u001b[39mFalse\u001b[39;49;00m)\n\u001b[1;32m   1117\u001b[0m     \u001b[39mfor\u001b[39;00m tp, records \u001b[39min\u001b[39;00m six\u001b[39m.\u001b[39miteritems(record_map):\n\u001b[1;32m   1118\u001b[0m         \u001b[39m# Generators are stateful, and it is possible that the tp / records\u001b[39;00m\n\u001b[1;32m   1119\u001b[0m         \u001b[39m# here may become stale during iteration -- i.e., we seek to a\u001b[39;00m\n\u001b[1;32m   1120\u001b[0m         \u001b[39m# different offset, pause consumption, or lose assignment.\u001b[39;00m\n\u001b[1;32m   1121\u001b[0m         \u001b[39mfor\u001b[39;00m record \u001b[39min\u001b[39;00m records:\n\u001b[1;32m   1122\u001b[0m             \u001b[39m# is_fetchable(tp) should handle assignment changes and offset\u001b[39;00m\n\u001b[1;32m   1123\u001b[0m             \u001b[39m# resets; for all other changes (e.g., seeks) we'll rely on the\u001b[39;00m\n\u001b[1;32m   1124\u001b[0m             \u001b[39m# outer function destroying the existing iterator/generator\u001b[39;00m\n\u001b[1;32m   1125\u001b[0m             \u001b[39m# via self._iterator = None\u001b[39;00m\n",
      "File \u001b[0;32m~/All_Repos_Env/ENV_SPARK/lib/python3.10/site-packages/kafka/consumer/group.py:655\u001b[0m, in \u001b[0;36mKafkaConsumer.poll\u001b[0;34m(self, timeout_ms, max_records, update_offsets)\u001b[0m\n\u001b[1;32m    653\u001b[0m remaining \u001b[39m=\u001b[39m timeout_ms\n\u001b[1;32m    654\u001b[0m \u001b[39mwhile\u001b[39;00m \u001b[39mTrue\u001b[39;00m:\n\u001b[0;32m--> 655\u001b[0m     records \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_poll_once(remaining, max_records, update_offsets\u001b[39m=\u001b[39;49mupdate_offsets)\n\u001b[1;32m    656\u001b[0m     \u001b[39mif\u001b[39;00m records:\n\u001b[1;32m    657\u001b[0m         \u001b[39mreturn\u001b[39;00m records\n",
      "File \u001b[0;32m~/All_Repos_Env/ENV_SPARK/lib/python3.10/site-packages/kafka/consumer/group.py:702\u001b[0m, in \u001b[0;36mKafkaConsumer._poll_once\u001b[0;34m(self, timeout_ms, max_records, update_offsets)\u001b[0m\n\u001b[1;32m    699\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_client\u001b[39m.\u001b[39mpoll(timeout_ms\u001b[39m=\u001b[39m\u001b[39m0\u001b[39m)\n\u001b[1;32m    701\u001b[0m timeout_ms \u001b[39m=\u001b[39m \u001b[39mmin\u001b[39m(timeout_ms, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_coordinator\u001b[39m.\u001b[39mtime_to_next_poll() \u001b[39m*\u001b[39m \u001b[39m1000\u001b[39m)\n\u001b[0;32m--> 702\u001b[0m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_client\u001b[39m.\u001b[39;49mpoll(timeout_ms\u001b[39m=\u001b[39;49mtimeout_ms)\n\u001b[1;32m    703\u001b[0m \u001b[39m# after the long poll, we should check whether the group needs to rebalance\u001b[39;00m\n\u001b[1;32m    704\u001b[0m \u001b[39m# prior to returning data so that the group can stabilize faster\u001b[39;00m\n\u001b[1;32m    705\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_coordinator\u001b[39m.\u001b[39mneed_rejoin():\n",
      "File \u001b[0;32m~/All_Repos_Env/ENV_SPARK/lib/python3.10/site-packages/kafka/client_async.py:602\u001b[0m, in \u001b[0;36mKafkaClient.poll\u001b[0;34m(self, timeout_ms, future)\u001b[0m\n\u001b[1;32m    599\u001b[0m             timeout \u001b[39m=\u001b[39m \u001b[39mmin\u001b[39m(timeout, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mconfig[\u001b[39m'\u001b[39m\u001b[39mretry_backoff_ms\u001b[39m\u001b[39m'\u001b[39m])\n\u001b[1;32m    600\u001b[0m         timeout \u001b[39m=\u001b[39m \u001b[39mmax\u001b[39m(\u001b[39m0\u001b[39m, timeout)  \u001b[39m# avoid negative timeouts\u001b[39;00m\n\u001b[0;32m--> 602\u001b[0m     \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_poll(timeout \u001b[39m/\u001b[39;49m \u001b[39m1000\u001b[39;49m)\n\u001b[1;32m    604\u001b[0m \u001b[39m# called without the lock to avoid deadlock potential\u001b[39;00m\n\u001b[1;32m    605\u001b[0m \u001b[39m# if handlers need to acquire locks\u001b[39;00m\n\u001b[1;32m    606\u001b[0m responses\u001b[39m.\u001b[39mextend(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_fire_pending_completed_requests())\n",
      "File \u001b[0;32m~/All_Repos_Env/ENV_SPARK/lib/python3.10/site-packages/kafka/client_async.py:634\u001b[0m, in \u001b[0;36mKafkaClient._poll\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    631\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_register_send_sockets()\n\u001b[1;32m    633\u001b[0m start_select \u001b[39m=\u001b[39m time\u001b[39m.\u001b[39mtime()\n\u001b[0;32m--> 634\u001b[0m ready \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_selector\u001b[39m.\u001b[39;49mselect(timeout)\n\u001b[1;32m    635\u001b[0m end_select \u001b[39m=\u001b[39m time\u001b[39m.\u001b[39mtime()\n\u001b[1;32m    636\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_sensors:\n",
      "File \u001b[0;32m/usr/lib/python3.10/selectors.py:469\u001b[0m, in \u001b[0;36mEpollSelector.select\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    467\u001b[0m ready \u001b[39m=\u001b[39m []\n\u001b[1;32m    468\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m--> 469\u001b[0m     fd_event_list \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_selector\u001b[39m.\u001b[39;49mpoll(timeout, max_ev)\n\u001b[1;32m    470\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mInterruptedError\u001b[39;00m:\n\u001b[1;32m    471\u001b[0m     \u001b[39mreturn\u001b[39;00m ready\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# consume the data from the kafka broker and save to json file   %%%% stop it after some time   10-15 sec %%%%%\n",
    "for message in Consumer_API:\n",
    "    message = message.value\n",
    "    with open('consumer_dump.json', 'a') as f:\n",
    "        json.dump(message, f)\n",
    "        # append new line  to end of  each json object\n",
    "        f.write('\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Consumer_API.topics()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# close consumer to free up resources after reading the messages\n",
    "# Consumer_API.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create topic\n",
    "# ! kafka-topics --bootstrap-server localhost:9092 --create --topic weather"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# delete topic\n",
    "# ! kafka-topics --bootstrap-server localhost:9092 --delete --topic weather"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Spark magic"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Unpacking and Exploding the  columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "weather_df = spark.read.json('consumer_dump.json')\n",
    "\n",
    "\n",
    "nested_json_type=type (weather_df.select(\"city\").take(1)[0][0]) # get the type of the nested json / dictionary object\n",
    "\n",
    "# function to expand the nested json object only\n",
    "def expand_json(df):\n",
    "    for c in df.columns:\n",
    "        if type(df.select(c).take(1)[0][0]) == nested_json_type:\n",
    "            df = df.select(\"*\", F.col(c+\".*\")).drop(c)\n",
    "            print(c)\n",
    "    return df\n",
    "# weather_df.show(26)\n",
    "# weather_df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#explode \"list\" column  as it is in list  of dictionary containing 40 records\n",
    "weather_df = weather_df.select (\"*\", F.explode(\"list\").alias(\"every_3_hr_weather\")).drop(\"list\")\n",
    "\n",
    "# weather_df.show(30)\n",
    "# weather_df.printSchema()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "city\n",
      "every_3_hr_weather\n"
     ]
    }
   ],
   "source": [
    "# LEVEL 1 Unpacking (city and every_3_hr_weather  column)\n",
    "weather_df=expand_json(weather_df)\n",
    "# weather_df.show(30)\n",
    "# weather_df.printSchema()\n",
    "\n",
    "# expand_json function can be replaced by   below  code :  and more ...\n",
    "# df_weather = df_weather.select(\"*\", F.col(\"city.*\")) \\\n",
    "#     .drop(\"city\")\n",
    "\n",
    "# df_weather = df_weather.select(\"*\", F.col(\"coord.*\")) \\\n",
    "#     .drop(\"coord\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "coord\n",
      "clouds\n",
      "main\n",
      "sys\n",
      "wind\n",
      "weather_dict\n"
     ]
    }
   ],
   "source": [
    "#explode weather columnn ,it is in list  of dictionary conataing 1 record only\n",
    "weather_df = weather_df.select (\"*\", F.explode(\"weather\").alias(\"weather_dict\")).drop(\"weather\")\n",
    "\n",
    "# LEVEL 2 Unpacking  (coord,clouds,main,rain,sys,wind,weather_dict column)\n",
    "weather_df=expand_json(weather_df)\n",
    "\n",
    "# rename column \"all\" to \"coludiness\"\n",
    "weather_df = weather_df.withColumnRenamed(\"all\", \"cloudiness\")\n",
    "\n",
    "# weather_df.show(30)\n",
    "# weather_df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------+----------+----------+----------+-------------------+---+----------+-------+-------+----------+----------+----------+--------+--------+---------+--------+--------+---+---+----+-----+-----------+------+\n",
      "|          name|   sunrise|    sunset|        dt|             dt_txt|pop|visibility|    lat|    lon|cloudiness|feels_like|grnd_level|humidity|pressure|sea_level|temp_max|temp_min|pod|deg|gust|speed|description|  main|\n",
      "+--------------+----------+----------+----------+-------------------+---+----------+-------+-------+----------+----------+----------+--------+--------+---------+--------+--------+---+---+----+-----+-----------+------+\n",
      "|Baudhatinchule|1668732107|1668770745|1668762000|2022-11-18 09:00:00|0.0|     10000|27.7167|85.3667|        14|    292.34|       871|      48|    1018|     1018|  293.04|  292.63|  d|190|1.17| 0.43| few clouds|Clouds|\n",
      "|Baudhatinchule|1668732107|1668770745|1668772800|2022-11-18 12:00:00|0.0|     10000|27.7167|85.3667|         9|     287.5|       869|      69|    1017|     1017|  288.14|  285.59|  n|229|1.43| 1.17|  clear sky| Clear|\n",
      "|Baudhatinchule|1668732107|1668770745|1668783600|2022-11-18 15:00:00|0.0|     10000|27.7167|85.3667|         0|    283.43|       871|      84|    1018|     1018|  284.09|  284.09|  n| 38|0.96| 0.48|  clear sky| Clear|\n",
      "|Baudhatinchule|1668732107|1668770745|1668794400|2022-11-18 18:00:00|0.0|     10000|27.7167|85.3667|         0|    282.81|       871|      84|    1018|     1018|  283.52|  283.52|  n| 95|1.22| 0.78|  clear sky| Clear|\n",
      "|Baudhatinchule|1668732107|1668770745|1668805200|2022-11-18 21:00:00|0.0|     10000|27.7167|85.3667|         0|    283.14|       870|      82|    1018|     1018|  283.14|  283.14|  n| 99|1.09| 0.62|  clear sky| Clear|\n",
      "|Baudhatinchule|1668732107|1668770745|1668816000|2022-11-19 00:00:00|0.0|     10000|27.7167|85.3667|         0|    282.85|       871|      79|    1018|     1018|  282.85|  282.85|  n| 98|1.26| 0.72|  clear sky| Clear|\n",
      "|Baudhatinchule|1668732107|1668770745|1668826800|2022-11-19 03:00:00|0.0|     10000|27.7167|85.3667|         0|    288.13|       874|      60|    1019|     1019|  288.93|  288.93|  d| 93| 0.8| 0.68|  clear sky| Clear|\n",
      "|Baudhatinchule|1668732107|1668770745|1668837600|2022-11-19 06:00:00|0.0|     10000|27.7167|85.3667|         0|    293.15|       874|      44|    1016|     1016|  293.87|  293.87|  d|284|1.24| 0.59|  clear sky| Clear|\n",
      "|Baudhatinchule|1668732107|1668770745|1668848400|2022-11-19 09:00:00|0.0|     10000|27.7167|85.3667|         0|    293.25|       871|      44|    1013|     1013|  293.96|  293.96|  d|285|1.67| 1.17|  clear sky| Clear|\n",
      "|Baudhatinchule|1668732107|1668770745|1668859200|2022-11-19 12:00:00|0.0|     10000|27.7167|85.3667|         0|    286.03|       869|      73|    1014|     1014|  286.71|  286.71|  n|296|1.16| 0.96|  clear sky| Clear|\n",
      "+--------------+----------+----------+----------+-------------------+---+----------+-------+-------+----------+----------+----------+--------+--------+---------+--------+--------+---+---+----+-----+-----------+------+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#  delete the column which are not required\n",
    "cleaned_weather_df = weather_df.drop(\"cod\",\"message\",\"cnt\",\"3h\",\"icon\",\"timezone\",\"population\",\"country\",\"id\",\"temp\",\"temp_kf\")\n",
    "\n",
    "# cleaned_weather_df.printSchema()\n",
    "cleaned_weather_df.show(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Questions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "# UDf to convert unix time stamp to date time  \n",
    "def unix_to_datetime(unix_time):\n",
    "    return datetime.fromtimestamp(unix_time).strftime('%Y-%m-%d %H:%M:%S')\n",
    "\n",
    "\n",
    "unix_to_datetime_udf = F.udf(unix_to_datetime, StringType())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------+-------------------+-------------------+-------------------+-------------------+---+----------+-------+-------+----------+----------+----------+--------+--------+---------+--------+--------+---+---+----+-----+-----------+------+\n",
      "|          name|            sunrise|             sunset|                 dt|             dt_txt|pop|visibility|    lat|    lon|cloudiness|feels_like|grnd_level|humidity|pressure|sea_level|temp_max|temp_min|pod|deg|gust|speed|description|  main|\n",
      "+--------------+-------------------+-------------------+-------------------+-------------------+---+----------+-------+-------+----------+----------+----------+--------+--------+---------+--------+--------+---+---+----+-----+-----------+------+\n",
      "|Baudhatinchule|2022-11-18 06:26:47|2022-11-18 17:10:45|2022-11-18 14:45:00|2022-11-18 09:00:00|0.0|     10000|27.7167|85.3667|        14|    292.34|       871|      48|    1018|     1018|  293.04|  292.63|  d|190|1.17| 0.43| few clouds|Clouds|\n",
      "|Baudhatinchule|2022-11-18 06:26:47|2022-11-18 17:10:45|2022-11-18 17:45:00|2022-11-18 12:00:00|0.0|     10000|27.7167|85.3667|         9|     287.5|       869|      69|    1017|     1017|  288.14|  285.59|  n|229|1.43| 1.17|  clear sky| Clear|\n",
      "|Baudhatinchule|2022-11-18 06:26:47|2022-11-18 17:10:45|2022-11-18 20:45:00|2022-11-18 15:00:00|0.0|     10000|27.7167|85.3667|         0|    283.43|       871|      84|    1018|     1018|  284.09|  284.09|  n| 38|0.96| 0.48|  clear sky| Clear|\n",
      "|Baudhatinchule|2022-11-18 06:26:47|2022-11-18 17:10:45|2022-11-18 23:45:00|2022-11-18 18:00:00|0.0|     10000|27.7167|85.3667|         0|    282.81|       871|      84|    1018|     1018|  283.52|  283.52|  n| 95|1.22| 0.78|  clear sky| Clear|\n",
      "|Baudhatinchule|2022-11-18 06:26:47|2022-11-18 17:10:45|2022-11-19 02:45:00|2022-11-18 21:00:00|0.0|     10000|27.7167|85.3667|         0|    283.14|       870|      82|    1018|     1018|  283.14|  283.14|  n| 99|1.09| 0.62|  clear sky| Clear|\n",
      "+--------------+-------------------+-------------------+-------------------+-------------------+---+----------+-------+-------+----------+----------+----------+--------+--------+---------+--------+--------+---+---+----+-----+-----------+------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# apply the UDF to the  dt,sunrise,sunset column\n",
    "Q1_weather_df = cleaned_weather_df.withColumn(\"dt\", unix_to_datetime_udf(\"dt\")).withColumn(\"sunrise\", unix_to_datetime_udf(\"sunrise\")).withColumn(\"sunset\", unix_to_datetime_udf(\"sunset\"))\n",
    "Q1_weather_df.show(5)\n",
    "# Q1_weather_df.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  Q1 Producer to save transformation to Kafka topic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "Q1_Producer = KafkaProducer(bootstrap_servers=[\n",
    "                         'localhost:9092'], value_serializer=lambda x: dumps(x).encode('utf-8'))\n",
    "\n",
    "\n",
    "for row in Q1_weather_df.rdd.collect():\n",
    "    # send data to kafka topic in schema and payload json format \n",
    "    Q1_Producer.send( 'Q1_date_time', {\"schema\": Schema_Q1, \"payload\": row.asDict()})\n",
    "    Q1_Producer.flush()  # flush the data to the kafka broker ( topic) and  make sure data  is sent to the kafka broker and  not lost in the buffer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Q1 connector config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "Q1_connector_name=\"Q1_date_time-sink\"\n",
    "config={\n",
    "    \"name\":Q1_connector_name ,\n",
    "    \"config\": {\n",
    "        \"connector.class\": \"io.confluent.connect.jdbc.JdbcSinkConnector\",\n",
    "        \"connection.url\": \"jdbc:postgresql://localhost:5432/Kafka_project\",\n",
    "        \"tasks.max\": \"1\", \n",
    "        \"topics\": \"Q1_date_time\",\n",
    "        \"insert.mode\": \"insert\",\n",
    "        \"connection.user\": \"amrit\",\n",
    "        \"connection.password\": \"1234\",\n",
    "        \"table.name.format\": \"Q1_date_time\",\n",
    "        \"auto.create\": \"true\",\n",
    "        \"auto.evolve\": \"true\",\n",
    "        \"value.converter\": \"org.apache.kafka.connect.json.JsonConverter\",\n",
    "        \"value.converter.schemas.enable\": \"true\",\n",
    "        \"auto.offset.reset\": \"earliest\"\n",
    "    }\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'name': 'Q1_date_time-sink', 'config': {'connector.class': 'io.confluent.connect.jdbc.JdbcSinkConnector', 'connection.url': 'jdbc:postgresql://localhost:5432/Kafka_project', 'tasks.max': '1', 'topics': 'Q1_date_time', 'insert.mode': 'insert', 'connection.user': 'amrit', 'connection.password': '1234', 'table.name.format': 'Q1_date_time', 'auto.create': 'true', 'auto.evolve': 'true', 'value.converter': 'org.apache.kafka.connect.json.JsonConverter', 'value.converter.schemas.enable': 'true', 'auto.offset.reset': 'earliest', 'name': 'Q1_date_time-sink'}, 'tasks': [], 'type': 'sink'}\n"
     ]
    }
   ],
   "source": [
    "# create sink connector\n",
    "response = requests.post(\"http://localhost:8083/connectors\", headers={\"Content-Type\": \"application/json\"}, data=json.dumps(config))\n",
    "\n",
    "print(response.json())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'name': 'Q1_date_time-sink', 'connector': {'state': 'RUNNING', 'worker_id': '127.0.1.1:8083'}, 'tasks': [{'id': 0, 'state': 'RUNNING', 'worker_id': '127.0.1.1:8083'}], 'type': 'sink'}\n"
     ]
    }
   ],
   "source": [
    "# connector status=\n",
    "response = requests.get(\"http://localhost:8083/connectors/\"+Q1_connector_name+\"/status\")\n",
    "print(response.json())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------+--------------+--------+------------+------------+--------------+\n",
      "|          name|max_wind_speed|max_temp|max_humidity|max_pressure|max_cloudiness|\n",
      "+--------------+--------------+--------+------------+------------+--------------+\n",
      "|      Basahiya|          2.91|  300.78|          76|        1017|            73|\n",
      "|Baudhatinchule|          1.59|  294.45|          86|        1019|            40|\n",
      "|     Bhaktapur|          1.46|  294.22|          86|        1019|            40|\n",
      "|     Bharatpur|          2.26|  300.23|          90|        1018|            12|\n",
      "|    Biratnagar|           3.2|  301.51|          74|        1016|            92|\n",
      "| Birendranagar|          2.27|  297.36|          83|        1019|            35|\n",
      "|       Birgunj|          2.94|  300.76|          77|        1017|            31|\n",
      "|        Butwāl|          2.39|  298.67|          86|        1018|            25|\n",
      "|     Dhangadhi|          3.01|  299.17|          75|        1018|            65|\n",
      "|        Dharān|          2.85|  299.35|          77|        1017|            73|\n",
      "+--------------+--------------+--------+------------+------------+--------------+\n",
      "only showing top 10 rows\n",
      "\n",
      "root\n",
      " |-- name: string (nullable = true)\n",
      " |-- max_wind_speed: double (nullable = true)\n",
      " |-- max_temp: double (nullable = true)\n",
      " |-- max_humidity: long (nullable = true)\n",
      " |-- max_pressure: long (nullable = true)\n",
      " |-- max_cloudiness: long (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# for each city , find highest wind speed , temperature , humidity , pressure , cloudiness \n",
    "Q2_weather_df = cleaned_weather_df. \\\n",
    "    groupBy(\"name\"). \\\n",
    "    agg(F.max(\"speed\").alias(\"max_wind_speed\"),F.max(\"temp_max\").alias(\"max_temp\"),F.max(\"humidity\").alias(\"max_humidity\")\n",
    "        ,F.max(\"pressure\").alias(\"max_pressure\"),F.max(\"cloudiness\").alias(\"max_cloudiness\")) .sort(\"name\")\n",
    "\n",
    "Q2_weather_df.show(10)\n",
    "Q2_weather_df.printSchema()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  Q2 Producer to save transformation to Kafka topic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "Q2_Producer = KafkaProducer(bootstrap_servers=[\n",
    "                         'localhost:9092'], value_serializer=lambda x: dumps(x).encode('utf-8'))\n",
    "\n",
    "\n",
    "for row in Q2_weather_df.rdd.collect():\n",
    "    # send data to kafka topic in schema and payload json format \n",
    "    Q2_Producer.send( 'Q2_max', {\"schema\": Schema_Q2, \"payload\": row.asDict()})\n",
    "    Q2_Producer.flush()  # flush the data to the kafka broker ( topic) and  make sure data  is sent to the kafka broker and  not lost in the buffer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Q2 connector config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "Q2_connector_name=\"Q2_max-sink\"\n",
    "config={\n",
    "    \"name\":Q2_connector_name ,\n",
    "    \"config\": {\n",
    "        \"connector.class\": \"io.confluent.connect.jdbc.JdbcSinkConnector\",\n",
    "        \"connection.url\": \"jdbc:postgresql://localhost:5432/Kafka_project\",\n",
    "        \"tasks.max\": \"1\", \n",
    "        \"topics\": \"Q2_max\",\n",
    "        \"insert.mode\": \"insert\",\n",
    "        \"connection.user\": \"amrit\",\n",
    "        \"connection.password\": \"1234\",\n",
    "        \"table.name.format\": \"Q2_max\",\n",
    "        \"auto.create\": \"true\",\n",
    "        \"auto.evolve\": \"true\",\n",
    "        \"value.converter\": \"org.apache.kafka.connect.json.JsonConverter\",\n",
    "        \"value.converter.schemas.enable\": \"true\",\n",
    "        \"auto.offset.reset\": \"earliest\"\n",
    "    }\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'name': 'Q2_max-sink', 'config': {'connector.class': 'io.confluent.connect.jdbc.JdbcSinkConnector', 'connection.url': 'jdbc:postgresql://localhost:5432/Kafka_project', 'tasks.max': '1', 'topics': 'Q2_max', 'insert.mode': 'insert', 'connection.user': 'amrit', 'connection.password': '1234', 'table.name.format': 'Q2_max', 'auto.create': 'true', 'auto.evolve': 'true', 'value.converter': 'org.apache.kafka.connect.json.JsonConverter', 'value.converter.schemas.enable': 'true', 'auto.offset.reset': 'earliest', 'name': 'Q2_max-sink'}, 'tasks': [], 'type': 'sink'}\n"
     ]
    }
   ],
   "source": [
    "# create sink connector\n",
    "response = requests.post(\"http://localhost:8083/connectors\", headers={\"Content-Type\": \"application/json\"}, data=json.dumps(config))\n",
    "\n",
    "print(response.json())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'name': 'Q2_max-sink', 'connector': {'state': 'RUNNING', 'worker_id': '127.0.1.1:8083'}, 'tasks': [{'id': 0, 'state': 'RUNNING', 'worker_id': '127.0.1.1:8083'}], 'type': 'sink'}\n"
     ]
    }
   ],
   "source": [
    "# connector status=\n",
    "response = requests.get(\"http://localhost:8083/connectors/\"+Q2_connector_name+\"/status\")\n",
    "print(response.json())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_day(dt_txt):\n",
    "    return dt_txt.split(\" \")[0]\n",
    "\n",
    "extract_day_udf = F.udf(extract_day, StringType())\n",
    "\n",
    "Q3_weather_df = cleaned_weather_df.withColumn(\"day\", extract_day_udf(\"dt_txt\"))\n",
    "\n",
    "#  for each city  , each day , list min temperature  at night time pod=n\n",
    "Q3_weather_df = Q3_weather_df.filter(Q3_weather_df.pod == \"n\").groupBy(\"name\",\"day\").agg(F.min(\"temp_min\").alias(\"min_temp_night\")).sort(\"name\",\"day\")\n",
    "\n",
    "# Q3_weather_df.show(5)\n",
    "# Q3_weather_df.printSchema()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  Q3 Producer to save transformation to Kafka topic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "Q3_Producer = KafkaProducer(bootstrap_servers=[\n",
    "                         'localhost:9092'], value_serializer=lambda x: dumps(x).encode('utf-8'))\n",
    "\n",
    "\n",
    "for row in Q3_weather_df.rdd.collect():\n",
    "    # send data to kafka topic in schema and payload json format \n",
    "    Q3_Producer.send( 'Q3_min_night', {\"schema\": Schema_Q3, \"payload\": row.asDict()})\n",
    "    Q3_Producer.flush()  # flush the data to the kafka broker ( topic) and  make sure data  is sent to the kafka broker and  not lost in the buffer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Q3 connector config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "Q3_connector_name=\"Q3_min_night-sink\"\n",
    "config={\n",
    "    \"name\":Q3_connector_name ,\n",
    "    \"config\": {\n",
    "        \"connector.class\": \"io.confluent.connect.jdbc.JdbcSinkConnector\",\n",
    "        \"connection.url\": \"jdbc:postgresql://localhost:5432/Kafka_project\",\n",
    "        \"tasks.max\": \"1\", \n",
    "        \"topics\": \"Q3_min_night\",\n",
    "        \"insert.mode\": \"insert\",\n",
    "        \"connection.user\": \"amrit\",\n",
    "        \"connection.password\": \"1234\",\n",
    "        \"table.name.format\": \"Q3_min_night\",\n",
    "        \"auto.create\": \"true\",\n",
    "        \"auto.evolve\": \"true\",\n",
    "        \"value.converter\": \"org.apache.kafka.connect.json.JsonConverter\",\n",
    "        \"value.converter.schemas.enable\": \"true\",\n",
    "        \"auto.offset.reset\": \"earliest\"\n",
    "    }\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'name': 'Q3_min_night-sink', 'config': {'connector.class': 'io.confluent.connect.jdbc.JdbcSinkConnector', 'connection.url': 'jdbc:postgresql://localhost:5432/Kafka_project', 'tasks.max': '1', 'topics': 'Q3_min_night', 'insert.mode': 'insert', 'connection.user': 'amrit', 'connection.password': '1234', 'table.name.format': 'Q3_min_night', 'auto.create': 'true', 'auto.evolve': 'true', 'value.converter': 'org.apache.kafka.connect.json.JsonConverter', 'value.converter.schemas.enable': 'true', 'auto.offset.reset': 'earliest', 'name': 'Q3_min_night-sink'}, 'tasks': [], 'type': 'sink'}\n"
     ]
    }
   ],
   "source": [
    "# create sink connector\n",
    "response = requests.post(\"http://localhost:8083/connectors\", headers={\"Content-Type\": \"application/json\"}, data=json.dumps(config))\n",
    "\n",
    "print(response.json())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'name': 'Q3_min_night-sink', 'connector': {'state': 'RUNNING', 'worker_id': '127.0.1.1:8083'}, 'tasks': [{'id': 0, 'state': 'RUNNING', 'worker_id': '127.0.1.1:8083'}], 'type': 'sink'}\n"
     ]
    }
   ],
   "source": [
    "# connector status=\n",
    "response = requests.get(\"http://localhost:8083/connectors/\"+Q3_connector_name+\"/status\")\n",
    "print(response.json())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.6 ('ENV_SPARK': venv)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "2214088c3d0387d9d6dcbb7c6a9ab667498cf089874e7aaa07c32225f40a1e68"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
