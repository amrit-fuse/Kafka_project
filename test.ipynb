{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ALL installs "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ! pip install requests\n",
    "# ! pip install kafka-python\n",
    "# ! pip install pyspark\n",
    "# !pip  install -r requirements.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip freeze > requirements.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ALL imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from allsecrets import *\n",
    "from coordinates import *\n",
    "from schemas import *\n",
    "import json,requests\n",
    "from json import dumps,loads \n",
    "from kafka import KafkaProducer, KafkaConsumer\n",
    "from datetime import datetime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "your 131072x1 screen size is bogus. expect trouble\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "22/11/18 14:25:58 WARN Utils: Your hostname, AMRIT resolves to a loopback address: 127.0.1.1; using 172.20.113.35 instead (on interface eth0)\n",
      "22/11/18 14:25:58 WARN Utils: Set SPARK_LOCAL_IP if you need to bind to another address\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "22/11/18 14:26:00 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession , functions as F\n",
    "from pyspark.sql.functions import udf,col,countDistinct,date_format,row_number\n",
    "from pyspark.sql.types import FloatType , StringType , IntegerType , StructType , StructField , TimestampType\n",
    "from pyspark.sql.window import Window \n",
    "\n",
    "spark = SparkSession.builder.appName('Weather')\\\n",
    "    .getOrCreate()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Json print function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def json_print(json_object):\n",
    "    text = json.dumps(json_object, sort_keys=True, indent=4) # sort_keys=True means sort the keys in alphabetical order\n",
    "    # indent 4 means 4 spaces for each indentation\n",
    "    print(text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1st Producer and consumer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Producer  to handle API response and send to Kafka topic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "Producer_API = KafkaProducer(bootstrap_servers=[\n",
    "                         'localhost:9092'], value_serializer=lambda x: dumps(x).encode('utf-8'))\n",
    "\n",
    "for lat, lon in lat_lon.items():\n",
    "    parameters = {'lat': lat, \"lon\": lon, \"appid\": open_weather_api}\n",
    "    weather_response = requests.get(\n",
    "        \"http://api.openweathermap.org/data/2.5/forecast\", params=parameters)\n",
    "    Producer_API.send('weather', value=weather_response.json())\n",
    "    Producer_API.flush()  # flush the data to the kafka broker ( topic) and  make sure data  is sent to the kafka broker and  not lost in the buffer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Consumer to read from Kafka and write to file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# if consumer_dump.json file exists, delete it\n",
    "import os\n",
    "if os.path.exists('consumer_dump.json'):\n",
    "    # os.remove('consumer_dump.json')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "Consumer_API = KafkaConsumer('weather', bootstrap_servers=['localhost:9092'], auto_offset_reset='earliest',\n",
    "                         enable_auto_commit=True, value_deserializer=lambda x: loads(x.decode('utf-8')))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# consume the data from the kafka broker and save to json file   %%%% stop it after some time   10-15 sec %%%%%\n",
    "for message in Consumer_API:\n",
    "    message = message.value\n",
    "    with open('consumer_dump.json', 'a') as f:\n",
    "        json.dump(message, f)\n",
    "        # append new line  to end of  each json object\n",
    "        f.write('\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Consumer_API.topics()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# close consumer to free up resources after reading the messages\n",
    "# Consumer_API.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create topic\n",
    "# ! kafka-topics --bootstrap-server localhost:9092 --create --topic weather"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# delete topic\n",
    "# ! kafka-topics --bootstrap-server localhost:9092 --delete --topic weather"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Spark magic"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Unpacking and Exploding the  columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "weather_df = spark.read.json('consumer_dump.json')\n",
    "\n",
    "\n",
    "nested_json_type=type (weather_df.select(\"city\").take(1)[0][0]) # get the type of the nested json / dictionary object\n",
    "\n",
    "# function to expand the nested json object only\n",
    "def expand_json(df):\n",
    "    for c in df.columns:\n",
    "        if type(df.select(c).take(1)[0][0]) == nested_json_type:\n",
    "            df = df.select(\"*\", F.col(c+\".*\")).drop(c)\n",
    "            print(c)\n",
    "    return df\n",
    "# weather_df.show(26)\n",
    "# weather_df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#explode \"list\" column  as it is in list  of dictionary containing 40 records\n",
    "weather_df = weather_df.select (\"*\", F.explode(\"list\").alias(\"every_3_hr_weather\")).drop(\"list\")\n",
    "\n",
    "# weather_df.show(30)\n",
    "# weather_df.printSchema()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "city\n",
      "every_3_hr_weather\n"
     ]
    }
   ],
   "source": [
    "# LEVEL 1 Unpacking (city and every_3_hr_weather  column)\n",
    "weather_df=expand_json(weather_df)\n",
    "# weather_df.show(30)\n",
    "# weather_df.printSchema()\n",
    "\n",
    "# expand_json function can be replaced by   below  code :  and more ...\n",
    "# df_weather = df_weather.select(\"*\", F.col(\"city.*\")) \\\n",
    "#     .drop(\"city\")\n",
    "\n",
    "# df_weather = df_weather.select(\"*\", F.col(\"coord.*\")) \\\n",
    "#     .drop(\"coord\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "coord\n",
      "clouds\n",
      "main\n",
      "sys\n",
      "wind\n",
      "weather_dict\n"
     ]
    }
   ],
   "source": [
    "#explode weather columnn ,it is in list  of dictionary conataing 1 record only\n",
    "weather_df = weather_df.select (\"*\", F.explode(\"weather\").alias(\"weather_dict\")).drop(\"weather\")\n",
    "\n",
    "# LEVEL 2 Unpacking  (coord,clouds,main,rain,sys,wind,weather_dict column)\n",
    "weather_df=expand_json(weather_df)\n",
    "\n",
    "# rename column \"all\" to \"coludiness\"\n",
    "weather_df = weather_df.withColumnRenamed(\"all\", \"cloudiness\")\n",
    "\n",
    "# weather_df.show(30)\n",
    "# weather_df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "#  delete the column which are not required\n",
    "cleaned_weather_df = weather_df.drop(\"cod\",\"message\",\"cnt\",\"3h\",\"icon\",\"timezone\",\"population\",\"country\",\"id\",\"temp\",\"temp_kf\")\n",
    "\n",
    "# cleaned_weather_df.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Questions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# UDf to convert unix time stamp to date time  \n",
    "def unix_to_datetime(unix_time):\n",
    "    return datetime.fromtimestamp(unix_time).strftime('%Y-%m-%d %H:%M:%S')\n",
    "\n",
    "\n",
    "unix_to_datetime_udf = F.udf(unix_to_datetime, StringType())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------+-------------------+-------------------+-------------------+-------------------+---+----------+-------+-------+----------+----------+----------+--------+--------+---------+--------+--------+---+---+----+-----+-----------+------+\n",
      "|          name|            sunrise|             sunset|                 dt|             dt_txt|pop|visibility|    lat|    lon|cloudiness|feels_like|grnd_level|humidity|pressure|sea_level|temp_max|temp_min|pod|deg|gust|speed|description|  main|\n",
      "+--------------+-------------------+-------------------+-------------------+-------------------+---+----------+-------+-------+----------+----------+----------+--------+--------+---------+--------+--------+---+---+----+-----+-----------+------+\n",
      "|Baudhatinchule|2022-11-18 06:26:47|2022-11-18 17:10:45|2022-11-18 14:45:00|2022-11-18 09:00:00|0.0|     10000|27.7167|85.3667|        14|    292.34|       871|      48|    1018|     1018|  293.04|  292.63|  d|190|1.17| 0.43| few clouds|Clouds|\n",
      "|Baudhatinchule|2022-11-18 06:26:47|2022-11-18 17:10:45|2022-11-18 17:45:00|2022-11-18 12:00:00|0.0|     10000|27.7167|85.3667|         9|     287.5|       869|      69|    1017|     1017|  288.14|  285.59|  n|229|1.43| 1.17|  clear sky| Clear|\n",
      "|Baudhatinchule|2022-11-18 06:26:47|2022-11-18 17:10:45|2022-11-18 20:45:00|2022-11-18 15:00:00|0.0|     10000|27.7167|85.3667|         0|    283.43|       871|      84|    1018|     1018|  284.09|  284.09|  n| 38|0.96| 0.48|  clear sky| Clear|\n",
      "|Baudhatinchule|2022-11-18 06:26:47|2022-11-18 17:10:45|2022-11-18 23:45:00|2022-11-18 18:00:00|0.0|     10000|27.7167|85.3667|         0|    282.81|       871|      84|    1018|     1018|  283.52|  283.52|  n| 95|1.22| 0.78|  clear sky| Clear|\n",
      "|Baudhatinchule|2022-11-18 06:26:47|2022-11-18 17:10:45|2022-11-19 02:45:00|2022-11-18 21:00:00|0.0|     10000|27.7167|85.3667|         0|    283.14|       870|      82|    1018|     1018|  283.14|  283.14|  n| 99|1.09| 0.62|  clear sky| Clear|\n",
      "+--------------+-------------------+-------------------+-------------------+-------------------+---+----------+-------+-------+----------+----------+----------+--------+--------+---------+--------+--------+---+---+----+-----+-----------+------+\n",
      "only showing top 5 rows\n",
      "\n",
      "root\n",
      " |-- name: string (nullable = true)\n",
      " |-- sunrise: string (nullable = true)\n",
      " |-- sunset: string (nullable = true)\n",
      " |-- dt: string (nullable = true)\n",
      " |-- dt_txt: string (nullable = true)\n",
      " |-- pop: double (nullable = true)\n",
      " |-- visibility: long (nullable = true)\n",
      " |-- lat: double (nullable = true)\n",
      " |-- lon: double (nullable = true)\n",
      " |-- cloudiness: long (nullable = true)\n",
      " |-- feels_like: double (nullable = true)\n",
      " |-- grnd_level: long (nullable = true)\n",
      " |-- humidity: long (nullable = true)\n",
      " |-- pressure: long (nullable = true)\n",
      " |-- sea_level: long (nullable = true)\n",
      " |-- temp_max: double (nullable = true)\n",
      " |-- temp_min: double (nullable = true)\n",
      " |-- pod: string (nullable = true)\n",
      " |-- deg: long (nullable = true)\n",
      " |-- gust: double (nullable = true)\n",
      " |-- speed: double (nullable = true)\n",
      " |-- description: string (nullable = true)\n",
      " |-- main: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# apply the UDF to the  dt,sunrise,sunset column\n",
    "Q1_weather_df = cleaned_weather_df.withColumn(\"dt\", unix_to_datetime_udf(\"dt\")).withColumn(\"sunrise\", unix_to_datetime_udf(\"sunrise\")).withColumn(\"sunset\", unix_to_datetime_udf(\"sunset\"))\n",
    "# Q1_weather_df.show(5)\n",
    "# Q1_weather_df.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  Q1 Producer to save transformation to Kafka topic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "Q1_Producer = KafkaProducer(bootstrap_servers=[\n",
    "                         'localhost:9092'], value_serializer=lambda x: dumps(x).encode('utf-8'))\n",
    "\n",
    "\n",
    "for row in Q1_weather_df.rdd.collect():\n",
    "    # send data to kafka topic in schema and payload json format \n",
    "    Q1_Producer.send( 'Q1_date_time', {\"schema\": Schema_Q1, \"payload\": row.asDict()})\n",
    "    Q1_Producer.flush()  # flush the data to the kafka broker ( topic) and  make sure data  is sent to the kafka broker and  not lost in the buffer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Q1 connector config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "Q1_connector_name=\"Q1_date_time-sink\"\n",
    "config={\n",
    "    \"name\":Q1_connector_name ,\n",
    "    \"config\": {\n",
    "        \"connector.class\": \"io.confluent.connect.jdbc.JdbcSinkConnector\",\n",
    "        \"connection.url\": \"jdbc:postgresql://localhost:5432/Kafka_project\",\n",
    "        \"tasks.max\": \"1\", \n",
    "        \"topics\": \"Q1_date_time\",\n",
    "        \"insert.mode\": \"insert\",\n",
    "        \"connection.user\": \"amrit\",\n",
    "        \"connection.password\": \"1234\",\n",
    "        \"table.name.format\": \"Q1_date_time\",\n",
    "        \"auto.create\": \"true\",\n",
    "        \"auto.evolve\": \"true\",\n",
    "        \"value.converter\": \"org.apache.kafka.connect.json.JsonConverter\",\n",
    "        \"value.converter.schemas.enable\": \"true\",\n",
    "        \"auto.offset.reset\": \"earliest\"\n",
    "    }\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'name': 'Q1_date_time-sink', 'config': {'connector.class': 'io.confluent.connect.jdbc.JdbcSinkConnector', 'connection.url': 'jdbc:postgresql://localhost:5432/Kafka_project', 'tasks.max': '1', 'topics': 'Q1_date_time', 'insert.mode': 'insert', 'connection.user': 'amrit', 'connection.password': '1234', 'table.name.format': 'Q1_date_time', 'auto.create': 'true', 'auto.evolve': 'true', 'value.converter': 'org.apache.kafka.connect.json.JsonConverter', 'value.converter.schemas.enable': 'true', 'auto.offset.reset': 'earliest', 'name': 'Q1_date_time-sink'}, 'tasks': [], 'type': 'sink'}\n"
     ]
    }
   ],
   "source": [
    "# create sink connector\n",
    "response = requests.post(\"http://localhost:8083/connectors\", headers={\"Content-Type\": \"application/json\"}, data=json.dumps(config))\n",
    "\n",
    "print(response.json())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'name': 'Q1_date_time-sink', 'connector': {'state': 'RUNNING', 'worker_id': '127.0.1.1:8083'}, 'tasks': [{'id': 0, 'state': 'RUNNING', 'worker_id': '127.0.1.1:8083'}], 'type': 'sink'}\n"
     ]
    }
   ],
   "source": [
    "# connector status=\n",
    "response = requests.get(\"http://localhost:8083/connectors/\"+Q1_connector_name+\"/status\")\n",
    "print(response.json())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------+--------------+--------+------------+------------+--------------+\n",
      "|          name|max_wind_speed|max_temp|max_humidity|max_pressure|max_cloudiness|\n",
      "+--------------+--------------+--------+------------+------------+--------------+\n",
      "|Baudhatinchule|          1.59|  294.45|          86|        1019|            40|\n",
      "|     Bhaktapur|          1.46|  294.22|          86|        1019|            40|\n",
      "|     Bharatpur|          2.26|  300.23|          90|        1018|             7|\n",
      "|    Biratnagar|           3.2|  301.51|          74|        1016|            92|\n",
      "| Birendranagar|          2.27|  297.36|          83|        1019|            23|\n",
      "|       Birgunj|          2.94|  300.76|          77|        1017|            31|\n",
      "|        Butwāl|          2.39|  298.67|          86|        1018|             7|\n",
      "|     Dhangadhi|          3.01|  299.17|          75|        1018|            46|\n",
      "|        Dharān|          2.85|  299.35|          77|        1017|            73|\n",
      "|      Gulariyā|          2.64|  299.38|          78|        1018|            35|\n",
      "+--------------+--------------+--------+------------+------------+--------------+\n",
      "only showing top 10 rows\n",
      "\n",
      "root\n",
      " |-- name: string (nullable = true)\n",
      " |-- max_wind_speed: double (nullable = true)\n",
      " |-- max_temp: double (nullable = true)\n",
      " |-- max_humidity: long (nullable = true)\n",
      " |-- max_pressure: long (nullable = true)\n",
      " |-- max_cloudiness: long (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# for each city , find highest wind speed , temperature , humidity , pressure , cloudiness \n",
    "Q2_weather_df = cleaned_weather_df. \\\n",
    "    groupBy(\"name\"). \\\n",
    "    agg(F.max(\"speed\").alias(\"max_wind_speed\"),F.max(\"temp_max\").alias(\"max_temp\"),F.max(\"humidity\").alias(\"max_humidity\")\n",
    "        ,F.max(\"pressure\").alias(\"max_pressure\"),F.max(\"cloudiness\").alias(\"max_cloudiness\")) .sort(\"name\")\n",
    "\n",
    "Q2_weather_df.show(10)\n",
    "Q2_weather_df.printSchema()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  Q2 Producer to save transformation to Kafka topic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "Q2_Producer = KafkaProducer(bootstrap_servers=[\n",
    "                         'localhost:9092'], value_serializer=lambda x: dumps(x).encode('utf-8'))\n",
    "\n",
    "\n",
    "for row in Q2_weather_df.rdd.collect():\n",
    "    # send data to kafka topic in schema and payload json format \n",
    "    Q2_Producer.send( 'Q2_max', {\"schema\": Schema_Q2, \"payload\": row.asDict()})\n",
    "    Q2_Producer.flush()  # flush the data to the kafka broker ( topic) and  make sure data  is sent to the kafka broker and  not lost in the buffer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Q2 connector config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "Q2_connector_name=\"Q2_max-sink\"\n",
    "config={\n",
    "    \"name\":Q2_connector_name ,\n",
    "    \"config\": {\n",
    "        \"connector.class\": \"io.confluent.connect.jdbc.JdbcSinkConnector\",\n",
    "        \"connection.url\": \"jdbc:postgresql://localhost:5432/Kafka_project\",\n",
    "        \"tasks.max\": \"1\", \n",
    "        \"topics\": \"Q2_max\",\n",
    "        \"insert.mode\": \"insert\",\n",
    "        \"connection.user\": \"amrit\",\n",
    "        \"connection.password\": \"1234\",\n",
    "        \"table.name.format\": \"Q2_max\",\n",
    "        \"auto.create\": \"true\",\n",
    "        \"auto.evolve\": \"true\",\n",
    "        \"value.converter\": \"org.apache.kafka.connect.json.JsonConverter\",\n",
    "        \"value.converter.schemas.enable\": \"true\",\n",
    "        \"auto.offset.reset\": \"earliest\"\n",
    "    }\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'name': 'Q2_max-sink', 'config': {'connector.class': 'io.confluent.connect.jdbc.JdbcSinkConnector', 'connection.url': 'jdbc:postgresql://localhost:5432/Kafka_project', 'tasks.max': '1', 'topics': 'Q2_max', 'insert.mode': 'insert', 'connection.user': 'amrit', 'connection.password': '1234', 'table.name.format': 'Q2_max', 'auto.create': 'true', 'auto.evolve': 'true', 'value.converter': 'org.apache.kafka.connect.json.JsonConverter', 'value.converter.schemas.enable': 'true', 'auto.offset.reset': 'earliest', 'name': 'Q2_max-sink'}, 'tasks': [], 'type': 'sink'}\n"
     ]
    }
   ],
   "source": [
    "# create sink connector\n",
    "response = requests.post(\"http://localhost:8083/connectors\", headers={\"Content-Type\": \"application/json\"}, data=json.dumps(config))\n",
    "\n",
    "print(response.json())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'name': 'Q2_max-sink', 'connector': {'state': 'RUNNING', 'worker_id': '127.0.1.1:8083'}, 'tasks': [{'id': 0, 'state': 'RUNNING', 'worker_id': '127.0.1.1:8083'}], 'type': 'sink'}\n"
     ]
    }
   ],
   "source": [
    "# connector status=\n",
    "response = requests.get(\"http://localhost:8083/connectors/\"+Q2_connector_name+\"/status\")\n",
    "print(response.json())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------+----------+--------------+\n",
      "|          name|       day|min_temp_night|\n",
      "+--------------+----------+--------------+\n",
      "|Baudhatinchule|2022-11-18|        283.14|\n",
      "|Baudhatinchule|2022-11-19|        282.85|\n",
      "|Baudhatinchule|2022-11-20|        283.51|\n",
      "|Baudhatinchule|2022-11-21|        283.15|\n",
      "|Baudhatinchule|2022-11-22|        283.43|\n",
      "+--------------+----------+--------------+\n",
      "only showing top 5 rows\n",
      "\n",
      "root\n",
      " |-- name: string (nullable = true)\n",
      " |-- day: string (nullable = true)\n",
      " |-- min_temp_night: double (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "def extract_day(dt_txt):\n",
    "    return dt_txt.split(\" \")[0]\n",
    "\n",
    "extract_day_udf = F.udf(extract_day, StringType())\n",
    "\n",
    "Q3_weather_df = cleaned_weather_df.withColumn(\"day\", extract_day_udf(\"dt_txt\"))\n",
    "\n",
    "#  for each city  , each day , list min temperature  at night time pod=n\n",
    "Q3_weather_df = Q3_weather_df.filter(Q3_weather_df.pod == \"n\").groupBy(\"name\",\"day\").agg(F.min(\"temp_min\").alias(\"min_temp_night\")).sort(\"name\",\"day\")\n",
    "\n",
    "Q3_weather_df.show(5)\n",
    "Q3_weather_df.printSchema()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  Q3 Producer to save transformation to Kafka topic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "Q3_Producer = KafkaProducer(bootstrap_servers=[\n",
    "                         'localhost:9092'], value_serializer=lambda x: dumps(x).encode('utf-8'))\n",
    "\n",
    "\n",
    "for row in Q3_weather_df.rdd.collect():\n",
    "    # send data to kafka topic in schema and payload json format \n",
    "    Q3_Producer.send( 'Q3_min_night', {\"schema\": Schema_Q3, \"payload\": row.asDict()})\n",
    "    Q3_Producer.flush()  # flush the data to the kafka broker ( topic) and  make sure data  is sent to the kafka broker and  not lost in the buffer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Q3 connector config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "Q3_connector_name=\"Q3_min_night-sink\"\n",
    "config={\n",
    "    \"name\":Q3_connector_name ,\n",
    "    \"config\": {\n",
    "        \"connector.class\": \"io.confluent.connect.jdbc.JdbcSinkConnector\",\n",
    "        \"connection.url\": \"jdbc:postgresql://localhost:5432/Kafka_project\",\n",
    "        \"tasks.max\": \"1\", \n",
    "        \"topics\": \"Q3_min_night\",\n",
    "        \"insert.mode\": \"insert\",\n",
    "        \"connection.user\": \"amrit\",\n",
    "        \"connection.password\": \"1234\",\n",
    "        \"table.name.format\": \"Q3_min_night\",\n",
    "        \"auto.create\": \"true\",\n",
    "        \"auto.evolve\": \"true\",\n",
    "        \"value.converter\": \"org.apache.kafka.connect.json.JsonConverter\",\n",
    "        \"value.converter.schemas.enable\": \"true\",\n",
    "        \"auto.offset.reset\": \"earliest\"\n",
    "    }\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'name': 'Q3_min_night-sink', 'config': {'connector.class': 'io.confluent.connect.jdbc.JdbcSinkConnector', 'connection.url': 'jdbc:postgresql://localhost:5432/Kafka_project', 'tasks.max': '1', 'topics': 'Q3_min_night', 'insert.mode': 'insert', 'connection.user': 'amrit', 'connection.password': '1234', 'table.name.format': 'Q3_min_night', 'auto.create': 'true', 'auto.evolve': 'true', 'value.converter': 'org.apache.kafka.connect.json.JsonConverter', 'value.converter.schemas.enable': 'true', 'auto.offset.reset': 'earliest', 'name': 'Q3_min_night-sink'}, 'tasks': [], 'type': 'sink'}\n"
     ]
    }
   ],
   "source": [
    "# create sink connector\n",
    "response = requests.post(\"http://localhost:8083/connectors\", headers={\"Content-Type\": \"application/json\"}, data=json.dumps(config))\n",
    "\n",
    "print(response.json())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'name': 'Q3_min_night-sink', 'connector': {'state': 'RUNNING', 'worker_id': '127.0.1.1:8083'}, 'tasks': [{'id': 0, 'state': 'RUNNING', 'worker_id': '127.0.1.1:8083'}], 'type': 'sink'}\n"
     ]
    }
   ],
   "source": [
    "# connector status=\n",
    "response = requests.get(\"http://localhost:8083/connectors/\"+Q3_connector_name+\"/status\")\n",
    "print(response.json())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.6 ('ENV_SPARK': venv)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "2214088c3d0387d9d6dcbb7c6a9ab667498cf089874e7aaa07c32225f40a1e68"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
