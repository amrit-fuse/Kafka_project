{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ALL installs "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ! pip install requests\"\n",
    "# ! pip install kafka-python\n",
    "# ! pip install pyspark\n",
    "# !pip  install -r requirements.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip freeze > requirements.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ALL imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from allsecrets import *\n",
    "from titles import *\n",
    "from coordinates import *\n",
    "import json,requests\n",
    "from json import dumps,loads \n",
    "from kafka import KafkaProducer, KafkaConsumer\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "your 131072x1 screen size is bogus. expect trouble\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "22/11/17 16:04:44 WARN Utils: Your hostname, AMRIT resolves to a loopback address: 127.0.1.1; using 172.20.115.244 instead (on interface eth0)\n",
      "22/11/17 16:04:44 WARN Utils: Set SPARK_LOCAL_IP if you need to bind to another address\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "22/11/17 16:04:46 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession , functions as F\n",
    "\n",
    "from pyspark.sql.functions import udf,col,countDistinct,date_format,row_number\n",
    "\n",
    "from pyspark.sql.types import FloatType , StringType , IntegerType , StructType , StructField , TimestampType\n",
    "\n",
    "from pyspark.sql.window import Window \n",
    "\n",
    "spark = SparkSession.builder.appName('Weather')\\\n",
    "    .getOrCreate()\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Json print function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def json_print(json_object):\n",
    "    text = json.dumps(json_object, sort_keys=True, indent=4) # sort_keys=True means sort the keys in alphabetical order\n",
    "    # indent 4 means 4 spaces for each indentation\n",
    "    print(text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Openweather 5 days 3 hour forcast"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# parameters = {'lat': '27.7167', \"lon\": '85.3667', \"appid\": open_weather_api}\n",
    "# sample_response = requests.get(\"http://api.openweathermap.org/data/2.5/forecast\", params=parameters)\n",
    "\n",
    "# # save the response as json file\n",
    "# with open('sample_response.json', 'w') as f:\n",
    "#     json.dump(sample_response.json(), f)\n",
    "\n",
    "# weather_rdd= spark.sparkContext.parallelize([json.dumps(sample_response.json()) ]) # convert to rdd\n",
    "\n",
    "# #convert to spark dataframe\n",
    "# weather_df = spark.read.json(weather_rdd) # convert to spark dataframe\n",
    "\n",
    "# schema_weather = weather_df.schema # get the schema of the dataframe\n",
    "\n",
    "# #  save  schema_weather  to .json file\n",
    "# with open('schema_weather.json', 'w') as f:\n",
    "#     json.dump(schema_weather.jsonValue(), f, indent=4)\n",
    "\n",
    "# #  load  schema_weather  from .json file\n",
    "# with open('schema_weather.json', 'r') as f:\n",
    "#     schema_weather1 = F.StructType.fromJson(json.load(f)) \n",
    "\n",
    "# # schema_weather1\n",
    "\n",
    "# # weather_df.printSchema()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# #api.openweathermap.org/data/2.5/forecast?lat={lat}&lon={lon}&appid={API key}\n",
    "\n",
    "# for lat ,lon in lat_lon.items():\n",
    "#     parameters = {'lat': lat, \"lon\": lon, \"appid\": open_weather_api}\n",
    "#     weather_response = requests.get(\"http://api.openweathermap.org/data/2.5/forecast\", params=parameters)\n",
    "#     print (weather_response.url)\n",
    "#     print(weather_response.status_code)\n",
    "#     # json_print(weather_response.json())\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Test produces and consumer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test producer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "producer = KafkaProducer(bootstrap_servers=[\n",
    "                         'localhost:9092'], value_serializer=lambda x: dumps(x).encode('utf-8'))\n",
    "                         # both key(header) and value are converted to json format\n",
    "\n",
    "for lat, lon in lat_lon.items():\n",
    "    parameters = {'lat': lat, \"lon\": lon, \"appid\": open_weather_api}\n",
    "    weather_response = requests.get(\n",
    "        \"http://api.openweathermap.org/data/2.5/forecast\", params=parameters)\n",
    "\n",
    "    producer.send('weather', value=weather_response.json())\n",
    "    producer.flush()  # flush the data to the kafka broker ( topic) and  make sure data  is sent to the kafka broker and  not lost in the buffer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test consumer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "Consumer = KafkaConsumer('weather', bootstrap_servers=['localhost:9092'], auto_offset_reset='earliest',\n",
    "                         enable_auto_commit=True, value_deserializer=lambda x: loads(x.decode('utf-8')))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn [17], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[39m# consume the data from the kafka broker and save to json file   %%%% stop it after some time   10-15 sec %%%%%\u001b[39;00m\n\u001b[0;32m----> 2\u001b[0m \u001b[39mfor\u001b[39;00m message \u001b[39min\u001b[39;00m Consumer:\n\u001b[1;32m      3\u001b[0m     message \u001b[39m=\u001b[39m message\u001b[39m.\u001b[39mvalue\n\u001b[1;32m      4\u001b[0m     \u001b[39mwith\u001b[39;00m \u001b[39mopen\u001b[39m(\u001b[39m'\u001b[39m\u001b[39mweather.json\u001b[39m\u001b[39m'\u001b[39m, \u001b[39m'\u001b[39m\u001b[39mw\u001b[39m\u001b[39m'\u001b[39m) \u001b[39mas\u001b[39;00m f:\n",
      "File \u001b[0;32m~/All_Repos_Env/ENV_SPARK/lib/python3.10/site-packages/kafka/consumer/group.py:1193\u001b[0m, in \u001b[0;36mKafkaConsumer.__next__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1191\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mnext_v1()\n\u001b[1;32m   1192\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m-> 1193\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mnext_v2()\n",
      "File \u001b[0;32m~/All_Repos_Env/ENV_SPARK/lib/python3.10/site-packages/kafka/consumer/group.py:1201\u001b[0m, in \u001b[0;36mKafkaConsumer.next_v2\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1199\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_iterator \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_message_generator_v2()\n\u001b[1;32m   1200\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m-> 1201\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mnext\u001b[39;49m(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_iterator)\n\u001b[1;32m   1202\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mStopIteration\u001b[39;00m:\n\u001b[1;32m   1203\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_iterator \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/All_Repos_Env/ENV_SPARK/lib/python3.10/site-packages/kafka/consumer/group.py:1116\u001b[0m, in \u001b[0;36mKafkaConsumer._message_generator_v2\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1114\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m_message_generator_v2\u001b[39m(\u001b[39mself\u001b[39m):\n\u001b[1;32m   1115\u001b[0m     timeout_ms \u001b[39m=\u001b[39m \u001b[39m1000\u001b[39m \u001b[39m*\u001b[39m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_consumer_timeout \u001b[39m-\u001b[39m time\u001b[39m.\u001b[39mtime())\n\u001b[0;32m-> 1116\u001b[0m     record_map \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mpoll(timeout_ms\u001b[39m=\u001b[39;49mtimeout_ms, update_offsets\u001b[39m=\u001b[39;49m\u001b[39mFalse\u001b[39;49;00m)\n\u001b[1;32m   1117\u001b[0m     \u001b[39mfor\u001b[39;00m tp, records \u001b[39min\u001b[39;00m six\u001b[39m.\u001b[39miteritems(record_map):\n\u001b[1;32m   1118\u001b[0m         \u001b[39m# Generators are stateful, and it is possible that the tp / records\u001b[39;00m\n\u001b[1;32m   1119\u001b[0m         \u001b[39m# here may become stale during iteration -- i.e., we seek to a\u001b[39;00m\n\u001b[1;32m   1120\u001b[0m         \u001b[39m# different offset, pause consumption, or lose assignment.\u001b[39;00m\n\u001b[1;32m   1121\u001b[0m         \u001b[39mfor\u001b[39;00m record \u001b[39min\u001b[39;00m records:\n\u001b[1;32m   1122\u001b[0m             \u001b[39m# is_fetchable(tp) should handle assignment changes and offset\u001b[39;00m\n\u001b[1;32m   1123\u001b[0m             \u001b[39m# resets; for all other changes (e.g., seeks) we'll rely on the\u001b[39;00m\n\u001b[1;32m   1124\u001b[0m             \u001b[39m# outer function destroying the existing iterator/generator\u001b[39;00m\n\u001b[1;32m   1125\u001b[0m             \u001b[39m# via self._iterator = None\u001b[39;00m\n",
      "File \u001b[0;32m~/All_Repos_Env/ENV_SPARK/lib/python3.10/site-packages/kafka/consumer/group.py:655\u001b[0m, in \u001b[0;36mKafkaConsumer.poll\u001b[0;34m(self, timeout_ms, max_records, update_offsets)\u001b[0m\n\u001b[1;32m    653\u001b[0m remaining \u001b[39m=\u001b[39m timeout_ms\n\u001b[1;32m    654\u001b[0m \u001b[39mwhile\u001b[39;00m \u001b[39mTrue\u001b[39;00m:\n\u001b[0;32m--> 655\u001b[0m     records \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_poll_once(remaining, max_records, update_offsets\u001b[39m=\u001b[39;49mupdate_offsets)\n\u001b[1;32m    656\u001b[0m     \u001b[39mif\u001b[39;00m records:\n\u001b[1;32m    657\u001b[0m         \u001b[39mreturn\u001b[39;00m records\n",
      "File \u001b[0;32m~/All_Repos_Env/ENV_SPARK/lib/python3.10/site-packages/kafka/consumer/group.py:702\u001b[0m, in \u001b[0;36mKafkaConsumer._poll_once\u001b[0;34m(self, timeout_ms, max_records, update_offsets)\u001b[0m\n\u001b[1;32m    699\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_client\u001b[39m.\u001b[39mpoll(timeout_ms\u001b[39m=\u001b[39m\u001b[39m0\u001b[39m)\n\u001b[1;32m    701\u001b[0m timeout_ms \u001b[39m=\u001b[39m \u001b[39mmin\u001b[39m(timeout_ms, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_coordinator\u001b[39m.\u001b[39mtime_to_next_poll() \u001b[39m*\u001b[39m \u001b[39m1000\u001b[39m)\n\u001b[0;32m--> 702\u001b[0m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_client\u001b[39m.\u001b[39;49mpoll(timeout_ms\u001b[39m=\u001b[39;49mtimeout_ms)\n\u001b[1;32m    703\u001b[0m \u001b[39m# after the long poll, we should check whether the group needs to rebalance\u001b[39;00m\n\u001b[1;32m    704\u001b[0m \u001b[39m# prior to returning data so that the group can stabilize faster\u001b[39;00m\n\u001b[1;32m    705\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_coordinator\u001b[39m.\u001b[39mneed_rejoin():\n",
      "File \u001b[0;32m~/All_Repos_Env/ENV_SPARK/lib/python3.10/site-packages/kafka/client_async.py:602\u001b[0m, in \u001b[0;36mKafkaClient.poll\u001b[0;34m(self, timeout_ms, future)\u001b[0m\n\u001b[1;32m    599\u001b[0m             timeout \u001b[39m=\u001b[39m \u001b[39mmin\u001b[39m(timeout, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mconfig[\u001b[39m'\u001b[39m\u001b[39mretry_backoff_ms\u001b[39m\u001b[39m'\u001b[39m])\n\u001b[1;32m    600\u001b[0m         timeout \u001b[39m=\u001b[39m \u001b[39mmax\u001b[39m(\u001b[39m0\u001b[39m, timeout)  \u001b[39m# avoid negative timeouts\u001b[39;00m\n\u001b[0;32m--> 602\u001b[0m     \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_poll(timeout \u001b[39m/\u001b[39;49m \u001b[39m1000\u001b[39;49m)\n\u001b[1;32m    604\u001b[0m \u001b[39m# called without the lock to avoid deadlock potential\u001b[39;00m\n\u001b[1;32m    605\u001b[0m \u001b[39m# if handlers need to acquire locks\u001b[39;00m\n\u001b[1;32m    606\u001b[0m responses\u001b[39m.\u001b[39mextend(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_fire_pending_completed_requests())\n",
      "File \u001b[0;32m~/All_Repos_Env/ENV_SPARK/lib/python3.10/site-packages/kafka/client_async.py:634\u001b[0m, in \u001b[0;36mKafkaClient._poll\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    631\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_register_send_sockets()\n\u001b[1;32m    633\u001b[0m start_select \u001b[39m=\u001b[39m time\u001b[39m.\u001b[39mtime()\n\u001b[0;32m--> 634\u001b[0m ready \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_selector\u001b[39m.\u001b[39;49mselect(timeout)\n\u001b[1;32m    635\u001b[0m end_select \u001b[39m=\u001b[39m time\u001b[39m.\u001b[39mtime()\n\u001b[1;32m    636\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_sensors:\n",
      "File \u001b[0;32m/usr/lib/python3.10/selectors.py:469\u001b[0m, in \u001b[0;36mEpollSelector.select\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    467\u001b[0m ready \u001b[39m=\u001b[39m []\n\u001b[1;32m    468\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m--> 469\u001b[0m     fd_event_list \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_selector\u001b[39m.\u001b[39;49mpoll(timeout, max_ev)\n\u001b[1;32m    470\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mInterruptedError\u001b[39;00m:\n\u001b[1;32m    471\u001b[0m     \u001b[39mreturn\u001b[39;00m ready\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# consume the data from the kafka broker and save to json file   %%%% stop it after some time   10-15 sec %%%%%\n",
    "for message in Consumer:\n",
    "    message = message.value\n",
    "    with open('weather.json', 'w') as f:\n",
    "        json.dump(message, f)\n",
    "        f.write(' ') # add space between each json object\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "# close consumer to free up resources after reading the messages\n",
    "# Consumer.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'_confluent-command',\n",
       " '_confluent-telemetry-metrics',\n",
       " '_confluent_balancer_api_state',\n",
       " '_confluent_balancer_broker_samples',\n",
       " '_confluent_balancer_partition_samples',\n",
       " '_schemas',\n",
       " 'connect-configs',\n",
       " 'connect-offsets',\n",
       " 'connect-status',\n",
       " 'weather'}"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Consumer.topics()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error while executing topic command : Topic 'weather' already exists.\n",
      "[2022-11-17 16:29:40,109] ERROR org.apache.kafka.common.errors.TopicExistsException: Topic 'weather' already exists.\n",
      " (kafka.admin.TopicCommand$)\n"
     ]
    }
   ],
   "source": [
    "# create topic\n",
    "! kafka-topics --bootstrap-server localhost:9092 --create --topic weather --replication-factor 1 --partitions 1\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "metadata": {},
   "outputs": [],
   "source": [
    "# delete topic\n",
    "! kafka-topics --bootstrap-server localhost:9092 --delete --topic cleaned_table"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Spark magic"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Unpacking and Exploding the  columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 224,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "weather_df = spark.read.json('weather.json')\n",
    "\n",
    "nested_json_type=type (weather_df.select(\"city\").take(1)[0][0]) # get the type of the nested json\n",
    "\n",
    "# function to expand the nested json object only\n",
    "def expand_json(df):\n",
    "    for c in df.columns:\n",
    "        if type(df.select(c).take(1)[0][0]) == nested_json_type:\n",
    "            df = df.select(\"*\", F.col(c+\".*\")).drop(c)\n",
    "            print(c)\n",
    "    return df\n",
    "    \n",
    "# weather_df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 225,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+---+---+-------+--------------------+\n",
      "|                city|cnt|cod|message|  every_3_hr_weather|\n",
      "+--------------------+---+---+-------+--------------------+\n",
      "|{{27.7167, 85.366...| 40|200|      0|{{42}, 1668686400...|\n",
      "|{{27.7167, 85.366...| 40|200|      0|{{37}, 1668697200...|\n",
      "|{{27.7167, 85.366...| 40|200|      0|{{46}, 1668708000...|\n",
      "|{{27.7167, 85.366...| 40|200|      0|{{47}, 1668718800...|\n",
      "|{{27.7167, 85.366...| 40|200|      0|{{42}, 1668729600...|\n",
      "+--------------------+---+---+-------+--------------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "\n",
    "#explode \"list\" column  as it is in list  of dictionary conataing 40 records\n",
    "weather_df = weather_df.select (\"*\", F.explode(\"list\").alias(\"every_3_hr_weather\")).drop(\"list\")\n",
    "\n",
    "weather_df.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 226,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "city\n",
      "every_3_hr_weather\n",
      "+---+---+-------+------------------+-------+-------+--------------+----------+----------+----------+--------+------+----------+-------------------+--------------------+----+------+---+----------+--------------------+-----------------+\n",
      "|cnt|cod|message|             coord|country|     id|          name|population|   sunrise|    sunset|timezone|clouds|        dt|             dt_txt|                main| pop|  rain|sys|visibility|             weather|             wind|\n",
      "+---+---+-------+------------------+-------+-------+--------------+----------+----------+----------+--------+------+----------+-------------------+--------------------+----+------+---+----------+--------------------+-----------------+\n",
      "| 40|200|      0|{27.7167, 85.3667}|     NP|1283647|Baudhatinchule|         0|1668645660|1668684366|   20700|  {42}|1668686400|2022-11-17 12:00:00|{289.69, 868, 62,...|0.32|{0.26}|{n}|     10000|[{light rain, 10n...|{231, 1.19, 1.14}|\n",
      "| 40|200|      0|{27.7167, 85.3667}|     NP|1283647|Baudhatinchule|         0|1668645660|1668684366|   20700|  {37}|1668697200|2022-11-17 15:00:00|{286.68, 869, 75,...| 0.2|{0.17}|{n}|     10000|[{light rain, 10n...| {217, 0.96, 0.7}|\n",
      "| 40|200|      0|{27.7167, 85.3667}|     NP|1283647|Baudhatinchule|         0|1668645660|1668684366|   20700|  {46}|1668708000|2022-11-17 18:00:00|{283.93, 870, 86,...| 0.2|{0.15}|{n}|     10000|[{light rain, 10n...|{158, 0.68, 0.14}|\n",
      "| 40|200|      0|{27.7167, 85.3667}|     NP|1283647|Baudhatinchule|         0|1668645660|1668684366|   20700|  {47}|1668718800|2022-11-17 21:00:00|{283.2, 869, 87, ...| 0.0|  null|{n}|     10000|[{scattered cloud...| {72, 1.06, 0.76}|\n",
      "| 40|200|      0|{27.7167, 85.3667}|     NP|1283647|Baudhatinchule|         0|1668645660|1668684366|   20700|  {42}|1668729600|2022-11-18 00:00:00|{282.48, 869, 85,...| 0.0|  null|{n}|     10000|[{scattered cloud...|  {78, 1.2, 0.93}|\n",
      "+---+---+-------+------------------+-------+-------+--------------+----------+----------+----------+--------+------+----------+-------------------+--------------------+----+------+---+----------+--------------------+-----------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# LEVEL 1 Unpacking (city and every_3_hr_weather  column)\n",
    "weather_df=expand_json(weather_df)\n",
    "weather_df.show(5)\n",
    "\n",
    "# expand_json function can be replaced by  :  and more ...\n",
    "# df_weather = df_weather.select(\"*\", F.col(\"city.*\")) \\\n",
    "#     .drop(\"city\")\n",
    "\n",
    "# df_weather = df_weather.select(\"*\", F.col(\"coord.*\")) \\\n",
    "#     .drop(\"coord\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 227,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "coord\n",
      "clouds\n",
      "main\n",
      "rain\n",
      "sys\n",
      "wind\n",
      "weather_dict\n",
      "+---+---+-------+-------+-------+--------------+----------+----------+----------+--------+----------+-------------------+----+----------+-------+-------+----------+----------+----------+--------+--------+---------+------+-------+--------+--------+----+---+---+----+-----+----------------+----+---+------+\n",
      "|cnt|cod|message|country|     id|          name|population|   sunrise|    sunset|timezone|        dt|             dt_txt| pop|visibility|    lat|    lon|cloudiness|feels_like|grnd_level|humidity|pressure|sea_level|  temp|temp_kf|temp_max|temp_min|  3h|pod|deg|gust|speed|     description|icon| id|  main|\n",
      "+---+---+-------+-------+-------+--------------+----------+----------+----------+--------+----------+-------------------+----+----------+-------+-------+----------+----------+----------+--------+--------+---------+------+-------+--------+--------+----+---+---+----+-----+----------------+----+---+------+\n",
      "| 40|200|      0|     NP|1283647|Baudhatinchule|         0|1668645660|1668684366|   20700|1668686400|2022-11-17 12:00:00|0.32|     10000|27.7167|85.3667|        42|    289.69|       868|      62|    1014|     1014| 290.3|    3.9|   290.3|   286.4|0.26|  n|231|1.19| 1.14|      light rain| 10n|500|  Rain|\n",
      "| 40|200|      0|     NP|1283647|Baudhatinchule|         0|1668645660|1668684366|   20700|1668697200|2022-11-17 15:00:00| 0.2|     10000|27.7167|85.3667|        37|    286.68|       869|      75|    1016|     1016|287.26|    2.5|  287.26|  284.76|0.17|  n|217|0.96|  0.7|      light rain| 10n|500|  Rain|\n",
      "| 40|200|      0|     NP|1283647|Baudhatinchule|         0|1668645660|1668684366|   20700|1668708000|2022-11-17 18:00:00| 0.2|     10000|27.7167|85.3667|        46|    283.93|       870|      86|    1016|     1016|284.49|    0.0|  284.49|  284.49|0.15|  n|158|0.68| 0.14|      light rain| 10n|500|  Rain|\n",
      "| 40|200|      0|     NP|1283647|Baudhatinchule|         0|1668645660|1668684366|   20700|1668718800|2022-11-17 21:00:00| 0.0|     10000|27.7167|85.3667|        47|     283.2|       869|      87|    1015|     1015|283.81|    0.0|  283.81|  283.81|null|  n| 72|1.06| 0.76|scattered clouds| 03n|802|Clouds|\n",
      "| 40|200|      0|     NP|1283647|Baudhatinchule|         0|1668645660|1668684366|   20700|1668729600|2022-11-18 00:00:00| 0.0|     10000|27.7167|85.3667|        42|    282.48|       869|      85|    1016|     1016| 283.2|    0.0|   283.2|   283.2|null|  n| 78| 1.2| 0.93|scattered clouds| 03n|802|Clouds|\n",
      "+---+---+-------+-------+-------+--------------+----------+----------+----------+--------+----------+-------------------+----+----------+-------+-------+----------+----------+----------+--------+--------+---------+------+-------+--------+--------+----+---+---+----+-----+----------------+----+---+------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#explode weather columnn ,it is in list  of dictionary conataing 1 record only\n",
    "weather_df = weather_df.select (\"*\", F.explode(\"weather\").alias(\"weather_dict\")).drop(\"weather\")\n",
    "\n",
    "# LEVEL 2 Unpacking  (coord,clouds,main,rain,sys,wind,weather_dict column)\n",
    "weather_df=expand_json(weather_df)\n",
    "\n",
    "\n",
    "# rename column all to coludiness\n",
    "weather_df = weather_df.withColumnRenamed(\"all\", \"cloudiness\")\n",
    "weather_df.show(5)\n",
    "\n",
    "# weather_df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 228,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- name: string (nullable = true)\n",
      " |-- sunrise: long (nullable = true)\n",
      " |-- sunset: long (nullable = true)\n",
      " |-- dt: long (nullable = true)\n",
      " |-- dt_txt: string (nullable = true)\n",
      " |-- pop: double (nullable = true)\n",
      " |-- visibility: long (nullable = true)\n",
      " |-- lat: double (nullable = true)\n",
      " |-- lon: double (nullable = true)\n",
      " |-- cloudiness: long (nullable = true)\n",
      " |-- feels_like: double (nullable = true)\n",
      " |-- grnd_level: long (nullable = true)\n",
      " |-- humidity: long (nullable = true)\n",
      " |-- sea_level: long (nullable = true)\n",
      " |-- temp: double (nullable = true)\n",
      " |-- temp_kf: double (nullable = true)\n",
      " |-- temp_max: double (nullable = true)\n",
      " |-- temp_min: double (nullable = true)\n",
      " |-- pod: string (nullable = true)\n",
      " |-- deg: long (nullable = true)\n",
      " |-- gust: double (nullable = true)\n",
      " |-- speed: double (nullable = true)\n",
      " |-- description: string (nullable = true)\n",
      " |-- main: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#  delete the column which is not required\n",
    "weather_df = weather_df.drop(\"cod\",\"message\",\"cnt\",\"pressure\",\"3h\",\"icon\",\"timezone\",\"population\",\"country\",\"id\")\n",
    "\n",
    "weather_df.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Important functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 229,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------+-------------------+-------------------+-------------------+-------------------+----+----------+-------+-------+----------+----------+----------+--------+---------+------+-------+--------+--------+---+---+----+-----+----------------+------+\n",
      "|          name|            sunrise|             sunset|                 dt|             dt_txt| pop|visibility|    lat|    lon|cloudiness|feels_like|grnd_level|humidity|sea_level|  temp|temp_kf|temp_max|temp_min|pod|deg|gust|speed|     description|  main|\n",
      "+--------------+-------------------+-------------------+-------------------+-------------------+----+----------+-------+-------+----------+----------+----------+--------+---------+------+-------+--------+--------+---+---+----+-----+----------------+------+\n",
      "|Baudhatinchule|2022-11-17 06:26:00|2022-11-17 17:11:06|2022-11-17 17:45:00|2022-11-17 12:00:00|0.32|     10000|27.7167|85.3667|        42|    289.69|       868|      62|     1014| 290.3|    3.9|   290.3|   286.4|  n|231|1.19| 1.14|      light rain|  Rain|\n",
      "|Baudhatinchule|2022-11-17 06:26:00|2022-11-17 17:11:06|2022-11-17 20:45:00|2022-11-17 15:00:00| 0.2|     10000|27.7167|85.3667|        37|    286.68|       869|      75|     1016|287.26|    2.5|  287.26|  284.76|  n|217|0.96|  0.7|      light rain|  Rain|\n",
      "|Baudhatinchule|2022-11-17 06:26:00|2022-11-17 17:11:06|2022-11-17 23:45:00|2022-11-17 18:00:00| 0.2|     10000|27.7167|85.3667|        46|    283.93|       870|      86|     1016|284.49|    0.0|  284.49|  284.49|  n|158|0.68| 0.14|      light rain|  Rain|\n",
      "|Baudhatinchule|2022-11-17 06:26:00|2022-11-17 17:11:06|2022-11-18 02:45:00|2022-11-17 21:00:00| 0.0|     10000|27.7167|85.3667|        47|     283.2|       869|      87|     1015|283.81|    0.0|  283.81|  283.81|  n| 72|1.06| 0.76|scattered clouds|Clouds|\n",
      "|Baudhatinchule|2022-11-17 06:26:00|2022-11-17 17:11:06|2022-11-18 05:45:00|2022-11-18 00:00:00| 0.0|     10000|27.7167|85.3667|        42|    282.48|       869|      85|     1016| 283.2|    0.0|   283.2|   283.2|  n| 78| 1.2| 0.93|scattered clouds|Clouds|\n",
      "|Baudhatinchule|2022-11-17 06:26:00|2022-11-17 17:11:06|2022-11-18 08:45:00|2022-11-18 03:00:00| 0.0|     10000|27.7167|85.3667|         7|    287.05|       873|      72|     1018|287.66|    0.0|  287.66|  287.66|  d|117|1.61| 1.43|       clear sky| Clear|\n",
      "|Baudhatinchule|2022-11-17 06:26:00|2022-11-17 17:11:06|2022-11-18 11:45:00|2022-11-18 06:00:00| 0.0|     10000|27.7167|85.3667|         4|    291.17|       873|      51|     1016|291.91|    0.0|  291.91|  291.91|  d|138|1.57| 1.13|       clear sky| Clear|\n",
      "|Baudhatinchule|2022-11-17 06:26:00|2022-11-17 17:11:06|2022-11-18 14:45:00|2022-11-18 09:00:00| 0.0|     10000|27.7167|85.3667|        11|    291.55|       871|      48|     1014|292.32|    0.0|  292.32|  292.32|  d|201|1.36| 0.76|      few clouds|Clouds|\n",
      "|Baudhatinchule|2022-11-17 06:26:00|2022-11-17 17:11:06|2022-11-18 17:45:00|2022-11-18 12:00:00| 0.0|     10000|27.7167|85.3667|        18|    285.16|       869|      75|     1015|285.87|    0.0|  285.87|  285.87|  n|229|1.24| 0.86|      few clouds|Clouds|\n",
      "|Baudhatinchule|2022-11-17 06:26:00|2022-11-17 17:11:06|2022-11-18 20:45:00|2022-11-18 15:00:00| 0.0|     10000|27.7167|85.3667|         0|    283.59|       871|      81|     1018| 284.3|    0.0|   284.3|   284.3|  n| 12|1.04| 0.48|       clear sky| Clear|\n",
      "|Baudhatinchule|2022-11-17 06:26:00|2022-11-17 17:11:06|2022-11-18 23:45:00|2022-11-18 18:00:00| 0.0|     10000|27.7167|85.3667|         0|    282.94|       871|      82|     1018|283.69|    0.0|  283.69|  283.69|  n| 86| 1.2| 0.38|       clear sky| Clear|\n",
      "|Baudhatinchule|2022-11-17 06:26:00|2022-11-17 17:11:06|2022-11-19 02:45:00|2022-11-18 21:00:00| 0.0|     10000|27.7167|85.3667|         0|    282.39|       870|      80|     1017|283.24|    0.0|  283.24|  283.24|  n|106| 1.2| 0.67|       clear sky| Clear|\n",
      "|Baudhatinchule|2022-11-17 06:26:00|2022-11-17 17:11:06|2022-11-19 05:45:00|2022-11-19 00:00:00| 0.0|     10000|27.7167|85.3667|         0|    282.86|       870|      77|     1018|282.86|    0.0|  282.86|  282.86|  n| 96|1.17| 0.75|       clear sky| Clear|\n",
      "|Baudhatinchule|2022-11-17 06:26:00|2022-11-17 17:11:06|2022-11-19 08:45:00|2022-11-19 03:00:00| 0.0|     10000|27.7167|85.3667|         0|    288.23|       874|      58|     1019|289.07|    0.0|  289.07|  289.07|  d| 91|1.02| 0.93|       clear sky| Clear|\n",
      "|Baudhatinchule|2022-11-17 06:26:00|2022-11-17 17:11:06|2022-11-19 11:45:00|2022-11-19 06:00:00| 0.0|     10000|27.7167|85.3667|         0|    293.36|       873|      43|     1016|294.09|    0.0|  294.09|  294.09|  d|256|1.31| 0.67|       clear sky| Clear|\n",
      "|Baudhatinchule|2022-11-17 06:26:00|2022-11-17 17:11:06|2022-11-19 14:45:00|2022-11-19 09:00:00| 0.0|     10000|27.7167|85.3667|         0|    293.42|       871|      44|     1013|294.12|    0.0|  294.12|  294.12|  d|280| 1.6| 0.83|       clear sky| Clear|\n",
      "|Baudhatinchule|2022-11-17 06:26:00|2022-11-17 17:11:06|2022-11-19 17:45:00|2022-11-19 12:00:00| 0.0|     10000|27.7167|85.3667|         0|     286.1|       869|      74|     1014|286.75|    0.0|  286.75|  286.75|  n|267|1.58| 1.35|       clear sky| Clear|\n",
      "|Baudhatinchule|2022-11-17 06:26:00|2022-11-17 17:11:06|2022-11-19 20:45:00|2022-11-19 15:00:00| 0.0|     10000|27.7167|85.3667|         1|    284.43|       870|      80|     1017|285.09|    0.0|  285.09|  285.09|  n|152|0.94| 0.16|       clear sky| Clear|\n",
      "|Baudhatinchule|2022-11-17 06:26:00|2022-11-17 17:11:06|2022-11-19 23:45:00|2022-11-19 18:00:00| 0.0|     10000|27.7167|85.3667|         1|    283.52|       869|      79|     1016|284.29|    0.0|  284.29|  284.29|  n| 59|1.17| 0.62|       clear sky| Clear|\n",
      "|Baudhatinchule|2022-11-17 06:26:00|2022-11-17 17:11:06|2022-11-20 02:45:00|2022-11-19 21:00:00| 0.0|     10000|27.7167|85.3667|        12|    283.07|       868|      78|     1015| 283.9|    0.0|   283.9|   283.9|  n| 98|1.27| 0.69|      few clouds|Clouds|\n",
      "|Baudhatinchule|2022-11-17 06:26:00|2022-11-17 17:11:06|2022-11-20 05:45:00|2022-11-20 00:00:00| 0.0|     10000|27.7167|85.3667|         6|    282.79|       868|      75|     1015|283.72|    0.0|  283.72|  283.72|  n|112|1.28| 0.61|       clear sky| Clear|\n",
      "|Baudhatinchule|2022-11-17 06:26:00|2022-11-17 17:11:06|2022-11-20 08:45:00|2022-11-20 03:00:00| 0.0|     10000|27.7167|85.3667|         5|    289.21|       872|      58|     1016|289.96|    0.0|  289.96|  289.96|  d|145|0.87| 0.81|       clear sky| Clear|\n",
      "|Baudhatinchule|2022-11-17 06:26:00|2022-11-17 17:11:06|2022-11-20 11:45:00|2022-11-20 06:00:00| 0.0|     10000|27.7167|85.3667|         2|    293.62|       872|      46|     1014|294.25|    0.0|  294.25|  294.25|  d|251|1.61|  0.7|       clear sky| Clear|\n",
      "|Baudhatinchule|2022-11-17 06:26:00|2022-11-17 17:11:06|2022-11-20 14:45:00|2022-11-20 09:00:00| 0.0|     10000|27.7167|85.3667|         0|    293.66|       870|      47|     1011|294.27|    0.0|  294.27|  294.27|  d|292|1.79|  1.1|       clear sky| Clear|\n",
      "|Baudhatinchule|2022-11-17 06:26:00|2022-11-17 17:11:06|2022-11-20 17:45:00|2022-11-20 12:00:00| 0.0|     10000|27.7167|85.3667|         2|     286.5|       868|      76|     1013|287.07|    0.0|  287.07|  287.07|  n|281|1.47| 1.28|       clear sky| Clear|\n",
      "|Baudhatinchule|2022-11-17 06:26:00|2022-11-17 17:11:06|2022-11-20 20:45:00|2022-11-20 15:00:00| 0.0|     10000|27.7167|85.3667|         0|    284.64|       869|      79|     1015|285.31|    0.0|  285.31|  285.31|  n|346|1.09| 0.66|       clear sky| Clear|\n",
      "|Baudhatinchule|2022-11-17 06:26:00|2022-11-17 17:11:06|2022-11-20 23:45:00|2022-11-20 18:00:00| 0.0|     10000|27.7167|85.3667|         1|    283.75|       868|      78|     1015|284.52|    0.0|  284.52|  284.52|  n|218|1.19| 0.19|       clear sky| Clear|\n",
      "|Baudhatinchule|2022-11-17 06:26:00|2022-11-17 17:11:06|2022-11-21 02:45:00|2022-11-20 21:00:00| 0.0|     10000|27.7167|85.3667|         0|    282.98|       867|      74|     1014|283.92|    0.0|  283.92|  283.92|  n| 22|0.97|  0.2|       clear sky| Clear|\n",
      "|Baudhatinchule|2022-11-17 06:26:00|2022-11-17 17:11:06|2022-11-21 05:45:00|2022-11-21 00:00:00| 0.0|     10000|27.7167|85.3667|         0|    282.47|       867|      69|     1014|283.57|    0.0|  283.57|  283.57|  n|101|1.07| 0.32|       clear sky| Clear|\n",
      "|Baudhatinchule|2022-11-17 06:26:00|2022-11-17 17:11:06|2022-11-21 08:45:00|2022-11-21 03:00:00| 0.0|     10000|27.7167|85.3667|         1|    289.19|       870|      50|     1014|290.13|    0.0|  290.13|  290.13|  d|186|1.05| 0.97|       clear sky| Clear|\n",
      "|Baudhatinchule|2022-11-17 06:26:00|2022-11-17 17:11:06|2022-11-21 11:45:00|2022-11-21 06:00:00| 0.0|     10000|27.7167|85.3667|         1|    293.65|       871|      43|     1012|294.35|    0.0|  294.35|  294.35|  d|265|1.91| 0.99|       clear sky| Clear|\n",
      "|Baudhatinchule|2022-11-17 06:26:00|2022-11-17 17:11:06|2022-11-21 14:45:00|2022-11-21 09:00:00| 0.0|     10000|27.7167|85.3667|         0|     293.7|       869|      47|     1010| 294.3|    0.0|   294.3|   294.3|  d|264|2.02| 1.68|       clear sky| Clear|\n",
      "|Baudhatinchule|2022-11-17 06:26:00|2022-11-17 17:11:06|2022-11-21 17:45:00|2022-11-21 12:00:00| 0.0|     10000|27.7167|85.3667|         0|    286.26|       868|      81|     1013|286.73|    0.0|  286.73|  286.73|  n|270|1.67| 1.41|       clear sky| Clear|\n",
      "|Baudhatinchule|2022-11-17 06:26:00|2022-11-17 17:11:06|2022-11-21 20:45:00|2022-11-21 15:00:00| 0.0|     10000|27.7167|85.3667|         0|    284.52|       869|      84|     1015|285.08|    0.0|  285.08|  285.08|  n|327|0.97| 0.59|       clear sky| Clear|\n",
      "|Baudhatinchule|2022-11-17 06:26:00|2022-11-17 17:11:06|2022-11-21 23:45:00|2022-11-21 18:00:00| 0.0|     10000|27.7167|85.3667|         0|    283.76|       869|      83|     1015|284.41|    0.0|  284.41|  284.41|  n|133|1.14| 0.25|       clear sky| Clear|\n",
      "|Baudhatinchule|2022-11-17 06:26:00|2022-11-17 17:11:06|2022-11-22 02:45:00|2022-11-21 21:00:00| 0.0|     10000|27.7167|85.3667|         0|    283.14|       867|      79|     1013|283.94|    0.0|  283.94|  283.94|  n| 94|1.14| 0.35|       clear sky| Clear|\n",
      "|Baudhatinchule|2022-11-17 06:26:00|2022-11-17 17:11:06|2022-11-22 05:45:00|2022-11-22 00:00:00| 0.0|     10000|27.7167|85.3667|        10|    282.49|       867|      74|     1013|283.47|    0.0|  283.47|  283.47|  n|124|1.07| 0.44|       clear sky| Clear|\n",
      "|Baudhatinchule|2022-11-17 06:26:00|2022-11-17 17:11:06|2022-11-22 08:45:00|2022-11-22 03:00:00| 0.0|     10000|27.7167|85.3667|         7|    288.56|       871|      60|     1015|289.32|    0.0|  289.32|  289.32|  d|163|1.06|  0.9|       clear sky| Clear|\n",
      "|Baudhatinchule|2022-11-17 06:26:00|2022-11-17 17:11:06|2022-11-22 11:45:00|2022-11-22 06:00:00| 0.0|     10000|27.7167|85.3667|         7|    293.03|       871|      47|     1013|293.69|    0.0|  293.69|  293.69|  d|282|1.67| 0.93|       clear sky| Clear|\n",
      "|Baudhatinchule|2022-11-17 06:26:00|2022-11-17 17:11:06|2022-11-22 14:45:00|2022-11-22 09:00:00| 0.0|     10000|27.7167|85.3667|         3|    293.28|       869|      47|     1010|293.92|    0.0|  293.92|  293.92|  d|280|1.89| 1.52|       clear sky| Clear|\n",
      "+--------------+-------------------+-------------------+-------------------+-------------------+----+----------+-------+-------+----------+----------+----------+--------+---------+------+-------+--------+--------+---+---+----+-----+----------------+------+\n",
      "\n",
      "root\n",
      " |-- name: string (nullable = true)\n",
      " |-- sunrise: string (nullable = true)\n",
      " |-- sunset: string (nullable = true)\n",
      " |-- dt: string (nullable = true)\n",
      " |-- dt_txt: string (nullable = true)\n",
      " |-- pop: double (nullable = true)\n",
      " |-- visibility: long (nullable = true)\n",
      " |-- lat: double (nullable = true)\n",
      " |-- lon: double (nullable = true)\n",
      " |-- cloudiness: long (nullable = true)\n",
      " |-- feels_like: double (nullable = true)\n",
      " |-- grnd_level: long (nullable = true)\n",
      " |-- humidity: long (nullable = true)\n",
      " |-- sea_level: long (nullable = true)\n",
      " |-- temp: double (nullable = true)\n",
      " |-- temp_kf: double (nullable = true)\n",
      " |-- temp_max: double (nullable = true)\n",
      " |-- temp_min: double (nullable = true)\n",
      " |-- pod: string (nullable = true)\n",
      " |-- deg: long (nullable = true)\n",
      " |-- gust: double (nullable = true)\n",
      " |-- speed: double (nullable = true)\n",
      " |-- description: string (nullable = true)\n",
      " |-- main: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# function to convert the unix timestamp to date and time \n",
    "def convert_unix_to_date_time(df):\n",
    "    for c in df.columns:\n",
    "        if c == \"dt\":\n",
    "            df = df.withColumn(c, F.from_unixtime(F.col(c)))\n",
    "        elif c == \"sunrise\":\n",
    "            df = df.withColumn(c, F.from_unixtime(F.col(c)))\n",
    "        elif c == \"sunset\":\n",
    "            df = df.withColumn(c, F.from_unixtime(F.col(c)))\n",
    "    return df\n",
    "\n",
    "# convert the unix timestamp to date and time\n",
    "weather_df = convert_unix_to_date_time(weather_df)\n",
    "\n",
    "weather_df.show(40)\n",
    "\n",
    "weather_df.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 230,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save weather_df 'schema to .json file\n",
    "\n",
    "schema_weather = weather_df.schema # get the schema of the dataframe    \n",
    "with open('schema_weather.json', 'w') as f:\n",
    "    json.dump(schema_weather.jsonValue(), f, indent=4)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 231,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load schema_weather from .json file\n",
    "with open('schema_weather.json', 'r') as f:\n",
    "    schema_weather_struct = F.StructType.fromJson(json.load(f))\n",
    "\n",
    "# schema_weather_struct"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 280,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save weather_df in schema and payload json format\n",
    "with open('weather_df.json', 'w') as f:\n",
    "    for row in weather_df.rdd.collect():\n",
    "        json.dump({\"schema\": schema_weather_struct.jsonValue(), \"payload\": row.asDict()}, f)\n",
    "        f.write(' ') \n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### manual schema defination"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 298,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define schema manually ## schema extracted from weather_df.json file wont'work\n",
    "schema_manual= {\n",
    "    \"type\": \"struct\",\n",
    "    \"fields\": [\n",
    "        {\n",
    "            \"field\": \"name\",\n",
    "            \"type\": \"string\",\n",
    "            \"nullable\": \"true\"\n",
    "        },\n",
    "        {\n",
    "            \"field\": \"sunrise\",\n",
    "            \"type\": \"string\",\n",
    "            \"nullable\": \"true\"\n",
    "        },\n",
    "        {\n",
    "            \"field\": \"sunset\",\n",
    "            \"type\": \"string\",\n",
    "            \"nullable\": \"true\"\n",
    "        },\n",
    "        {\n",
    "            \"field\": \"dt\",\n",
    "            \"type\": \"string\",\n",
    "            \"nullable\": \"true\"\n",
    "        },\n",
    "        {\n",
    "            \"field\": \"dt_txt\",\n",
    "            \"type\": \"string\",\n",
    "            \"nullable\": \"true\"\n",
    "        },\n",
    "        {\n",
    "            \"field\": \"pop\",\n",
    "            \"type\": \"double\",\n",
    "            \"nullable\": \"true\"\n",
    "        },\n",
    "        {\n",
    "            \"field\": \"visibility\",\n",
    "            \"type\": \"int64\",\n",
    "            \"nullable\": \"true\"\n",
    "        },\n",
    "        {\n",
    "            \"field\": \"lat\",\n",
    "            \"type\": \"double\",\n",
    "            \"nullable\": \"true\"\n",
    "        },\n",
    "        {\n",
    "            \"field\": \"lon\",\n",
    "            \"type\": \"double\",\n",
    "            \"nullable\": \"true\"\n",
    "        },\n",
    "        {\n",
    "            \"field\": \"cloudiness\",\n",
    "            \"type\": \"int64\",\n",
    "            \"nullable\": \"true\"\n",
    "        },\n",
    "        {\n",
    "            \"field\": \"feels_like\",\n",
    "            \"type\": \"double\",\n",
    "            \"nullable\": \"true\"\n",
    "        },\n",
    "        {\n",
    "            \"field\": \"grnd_level\",\n",
    "            \"type\": \"int64\",\n",
    "            \"nullable\": \"true\"\n",
    "        },\n",
    "        {\n",
    "            \"field\": \"humidity\",\n",
    "            \"type\": \"int64\",\n",
    "            \"nullable\": \"true\"\n",
    "        },\n",
    "        {\n",
    "            \"field\": \"sea_level\",\n",
    "            \"type\": \"int64\",\n",
    "            \"nullable\": \"true\"\n",
    "        },\n",
    "        {\n",
    "            \"field\": \"temp\",\n",
    "            \"type\": \"double\",\n",
    "            \"nullable\": \"true\"\n",
    "        },\n",
    "        {\n",
    "            \"field\": \"temp_kf\",\n",
    "            \"type\": \"double\",\n",
    "            \"nullable\": \"true\"\n",
    "        },\n",
    "        {\n",
    "            \"field\": \"temp_max\",\n",
    "            \"type\": \"double\",\n",
    "            \"nullable\": \"true\"\n",
    "        },\n",
    "        {\n",
    "            \"field\": \"temp_min\",\n",
    "            \"type\": \"double\",\n",
    "            \"nullable\": \"true\"\n",
    "        },\n",
    "        {\n",
    "            \"field\": \"pod\",\n",
    "            \"type\": \"string\",\n",
    "            \"nullable\": \"true\"\n",
    "        },\n",
    "        {\n",
    "            \"field\": \"deg\",\n",
    "            \"type\": \"int64\",\n",
    "            \"nullable\": \"true\"\n",
    "        },\n",
    "        {\n",
    "            \"field\": \"gust\",\n",
    "            \"type\": \"double\",\n",
    "            \"nullable\": \"true\"\n",
    "        },\n",
    "        {\n",
    "            \"field\": \"speed\",\n",
    "            \"type\": \"double\",\n",
    "            \"nullable\": \"true\"\n",
    "        },\n",
    "        {\n",
    "            \"field\": \"description\",\n",
    "            \"type\": \"string\",\n",
    "            \"nullable\": \"true\"\n",
    "        },\n",
    "        {\n",
    "            \"field\": \"main\",\n",
    "            \"type\": \"string\",\n",
    "            \"nullable\": \"true\"\n",
    "        }\n",
    "    ]\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 307,
   "metadata": {},
   "outputs": [],
   "source": [
    "Producer_save_table = KafkaProducer(bootstrap_servers=[\n",
    "                         'localhost:9092'], value_serializer=lambda x: dumps(x).encode('utf-8'))\n",
    "# Producer_save_table = KafkaProducer(bootstrap_servers=['localhost:9092'])\n",
    "\n",
    "# for row in weather_df.rdd.collect():\n",
    "#     Producer_save_table.send('cleaned_table', row.asDict())\n",
    "#     Producer_save_table.flush()  # flush the data to the kafka broker ( topic) and  make sure data  is sent to the kafka broker and  not lost in the buffer\n",
    "\n",
    "\n",
    "\n",
    "for row in weather_df.rdd.collect():\n",
    "    # send data to kafka topic in schema and payload json format \n",
    "    Producer_save_table.send( 'cleaned_table', {\"schema\": schema_manual, \"payload\": row.asDict()})\n",
    "    Producer_save_table.flush()  # flush the data to the kafka broker ( topic) and  make sure data  is sent to the kafka broker and  not lost in the buffer\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 300,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # read and send weather_df.json file to kafka topic\n",
    "# with open('weather_df.json', 'r') as f:\n",
    "#     for line in f:\n",
    "#         Producer_save_table.send('cleaned_table', value=line)\n",
    "#         Producer_save_table.flush()  # flush the data to the kafka broker ( topic) and  make sure data  is sent to the kafka broker and  not lost in the buffer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 301,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# producer_cleaned_table = KafkaProducer(bootstrap_servers=[ 'localhost:9092'], value_serializer=lambda x: dumps(x).encode('utf-8'))\n",
    "\n",
    "# with open('cleaned_table.json', 'r') as f:\n",
    "#     for line in f:\n",
    "#         producer_cleaned_table.send('cleaned_table', line)\n",
    "#         producer_cleaned_table.flush()  # flush the data to the kafka broker ( topic) and  make sure data  is sent to the kafka broker and  not lost in the buffer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 308,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "connector_name=\"cleaned_table-sink-connector\"\n",
    "config={\n",
    "    \"name\":connector_name ,\n",
    "    \"config\": {\n",
    "        \"connector.class\": \"io.confluent.connect.jdbc.JdbcSinkConnector\",\n",
    "        \"connection.url\": \"jdbc:postgresql://localhost:5432/sink1\",\n",
    "        \"tasks.max\": \"1\",\n",
    "        \"topics\": \"cleaned_table\",\n",
    "        \"insert.mode\": \"insert\",\n",
    "        \"connection.user\": \"amrit\",\n",
    "        \"connection.password\": \"1234\",\n",
    "        \"table.name.format\": \"cleaned_table\",\n",
    "        \"auto.create\": \"true\",\n",
    "        \"auto.evolve\": \"true\",\n",
    "        \"value.converter\": \"org.apache.kafka.connect.json.JsonConverter\",\n",
    "        \"value.converter.schemas.enable\": \"true\",\n",
    "    }\n",
    "\n",
    "}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 309,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'name': 'cleaned_table-sink-connector', 'config': {'connector.class': 'io.confluent.connect.jdbc.JdbcSinkConnector', 'connection.url': 'jdbc:postgresql://localhost:5432/sink1', 'tasks.max': '1', 'topics': 'cleaned_table', 'insert.mode': 'insert', 'connection.user': 'amrit', 'connection.password': '1234', 'table.name.format': 'cleaned_table', 'auto.create': 'true', 'auto.evolve': 'true', 'value.converter': 'org.apache.kafka.connect.json.JsonConverter', 'value.converter.schemas.enable': 'true', 'name': 'cleaned_table-sink-connector'}, 'tasks': [], 'type': 'sink'}\n"
     ]
    }
   ],
   "source": [
    "# create sink connector\n",
    "response = requests.post(\"http://localhost:8083/connectors\", headers={\"Content-Type\": \"application/json\"}, data=json.dumps(config))\n",
    "\n",
    "print(response.json())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 310,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'name': 'cleaned_table-sink-connector', 'connector': {'state': 'RUNNING', 'worker_id': '127.0.1.1:8083'}, 'tasks': [{'id': 0, 'state': 'RUNNING', 'worker_id': '127.0.1.1:8083'}], 'type': 'sink'}\n"
     ]
    }
   ],
   "source": [
    "# connector status=\n",
    "response = requests.get(\"http://localhost:8083/connectors/\"+connector_name+\"/status\")\n",
    "print(response.json())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[]\n"
     ]
    }
   ],
   "source": [
    "#list all connectors\n",
    "response = requests.get(\"http://localhost:8083/connectors\")\n",
    "print(response.json())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 306,
   "metadata": {},
   "outputs": [],
   "source": [
    "# delete all the connector\n",
    "response = requests.get(\"http://localhost:8083/connectors\")\n",
    "for connector in response.json():\n",
    "    response = requests.delete(\"http://localhost:8083/connectors/\"+connector)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 305,
   "metadata": {},
   "outputs": [],
   "source": [
    "# delete topic\n",
    "! kafka-topics --bootstrap-server localhost:9092 --delete --topic cleaned_table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.6 ('ENV_SPARK': venv)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "2214088c3d0387d9d6dcbb7c6a9ab667498cf089874e7aaa07c32225f40a1e68"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
