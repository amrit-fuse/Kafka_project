{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ALL installs "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ! pip install requests\"\n",
    "# ! pip install kafka-python\n",
    "# ! pip install pyspark\n",
    "# !pip  install -r requirements.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip freeze > requirements.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ALL imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from allsecrets import *\n",
    "from titles import *\n",
    "from coordinates import *\n",
    "import json,requests\n",
    "from json import dumps,loads \n",
    "from kafka import KafkaProducer, KafkaConsumer\n",
    "import kafkaconnect"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "your 131072x1 screen size is bogus. expect trouble\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "22/11/16 16:54:08 WARN Utils: Your hostname, AMRIT resolves to a loopback address: 127.0.1.1; using 172.19.40.176 instead (on interface eth0)\n",
      "22/11/16 16:54:08 WARN Utils: Set SPARK_LOCAL_IP if you need to bind to another address\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "22/11/16 16:54:09 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession , functions as F\n",
    "\n",
    "from pyspark.sql.functions import udf,col,countDistinct,date_format,row_number\n",
    "\n",
    "from pyspark.sql.types import FloatType , StringType , IntegerType , StructType , StructField , TimestampType\n",
    "\n",
    "from pyspark.sql.window import Window \n",
    "\n",
    "spark = SparkSession.builder.appName('Weather')\\\n",
    "    .getOrCreate()\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Json print function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def json_print(json_object):\n",
    "    text = json.dumps(json_object, sort_keys=True, indent=4) # sort_keys=True means sort the keys in alphabetical order\n",
    "    # indent 4 means 4 spaces for each indentation\n",
    "    print(text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Openweather 5 days 3 hour forcast"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "parameters = {'lat': '27.7167', \"lon\": '85.3667', \"appid\": open_weather_api}\n",
    "sample_response = requests.get(\"http://api.openweathermap.org/data/2.5/forecast\", params=parameters)\n",
    "\n",
    "# save the response as json file\n",
    "with open('sample_response.json', 'w') as f:\n",
    "    json.dump(sample_response.json(), f)\n",
    "\n",
    "    \n",
    "\n",
    "\n",
    "weather_rdd= spark.sparkContext.parallelize([json.dumps(sample_response.json()) ]) # convert to rdd\n",
    "\n",
    "#convert to spark dataframe\n",
    "weather_df = spark.read.json(weather_rdd) # convert to spark dataframe\n",
    "\n",
    "schema_weather = weather_df.schema # get the schema of the dataframe\n",
    "\n",
    "#  save  schema_weather  to .json file\n",
    "with open('schema_weather.json', 'w') as f:\n",
    "    json.dump(schema_weather.jsonValue(), f, indent=4)\n",
    "\n",
    "#  load  schema_weather  from .json file\n",
    "with open('schema_weather.json', 'r') as f:\n",
    "    schema_weather1 = F.StructType.fromJson(json.load(f)) \n",
    "\n",
    "# schema_weather1\n",
    "\n",
    "# weather_df.printSchema()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "http://api.openweathermap.org/data/2.5/forecast?lat=27.7167&lon=85.3667&appid=8cf7d4ce0b67a86cf3b125bc5c552a6d\n",
      "200\n"
     ]
    }
   ],
   "source": [
    "#api.openweathermap.org/data/2.5/forecast?lat={lat}&lon={lon}&appid={API key}\n",
    "\n",
    "for lat ,lon in lat_lon.items():\n",
    "    parameters = {'lat': lat, \"lon\": lon, \"appid\": open_weather_api}\n",
    "    weather_response = requests.get(\"http://api.openweathermap.org/data/2.5/forecast\", params=parameters)\n",
    "    print (weather_response.url)\n",
    "    print(weather_response.status_code)\n",
    "    # json_print(weather_response.json())\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Test produces and consumer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test producer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# producer = KafkaProducer(bootstrap_servers=[\n",
    "#                          'localhost:9092'], value_serializer=lambda x: dumps(x).encode('utf-8'), key_serializer=lambda x: dumps(x).encode('utf-8'))\n",
    "#                          # both key(header) and value are converted to json format\n",
    "\n",
    "producer = KafkaProducer(bootstrap_servers=[\n",
    "                         'localhost:9092'], value_serializer=lambda x: dumps(x).encode('utf-8'))\n",
    "                         # both key(header) and value are converted to json format\n",
    "\n",
    "for lat, lon in lat_lon.items():\n",
    "    parameters = {'lat': lat, \"lon\": lon, \"appid\": open_weather_api}\n",
    "    weather_response = requests.get(\n",
    "        \"http://api.openweathermap.org/data/2.5/forecast\", params=parameters)\n",
    "\n",
    "    producer.send('weather', value=weather_response.json())\n",
    "    producer.flush()  # flush the data to the kafka broker ( topic) and  make sure data  is sent to the kafka broker and  not lost in the buffer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test consumer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create consumer\n",
    "# Consumer = KafkaConsumer('weather', bootstrap_servers=['localhost:9092'], auto_offset_reset='earliest',\n",
    "#                          enable_auto_commit=True, value_deserializer=lambda x: loads(x.decode('utf-8'), key_deserializer=lambda x: loads(x.decode('utf-8'))))\n",
    "\n",
    "\n",
    "Consumer = KafkaConsumer('weather', bootstrap_servers=['localhost:9092'], auto_offset_reset='earliest',\n",
    "                         enable_auto_commit=True, value_deserializer=lambda x: loads(x.decode('utf-8')))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn [38], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[39m# consume the data from the kafka broker and save to json file\u001b[39;00m\n\u001b[0;32m----> 2\u001b[0m \u001b[39mfor\u001b[39;00m message \u001b[39min\u001b[39;00m Consumer:\n\u001b[1;32m      3\u001b[0m     message \u001b[39m=\u001b[39m message\u001b[39m.\u001b[39mvalue\n\u001b[1;32m      4\u001b[0m     \u001b[39mwith\u001b[39;00m \u001b[39mopen\u001b[39m(\u001b[39m'\u001b[39m\u001b[39mweather.json\u001b[39m\u001b[39m'\u001b[39m, \u001b[39m'\u001b[39m\u001b[39ma\u001b[39m\u001b[39m'\u001b[39m) \u001b[39mas\u001b[39;00m f:\n",
      "File \u001b[0;32m~/All_Repos_Env/ENV_SPARK/lib/python3.10/site-packages/kafka/consumer/group.py:1193\u001b[0m, in \u001b[0;36mKafkaConsumer.__next__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1191\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mnext_v1()\n\u001b[1;32m   1192\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m-> 1193\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mnext_v2()\n",
      "File \u001b[0;32m~/All_Repos_Env/ENV_SPARK/lib/python3.10/site-packages/kafka/consumer/group.py:1201\u001b[0m, in \u001b[0;36mKafkaConsumer.next_v2\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1199\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_iterator \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_message_generator_v2()\n\u001b[1;32m   1200\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m-> 1201\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mnext\u001b[39;49m(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_iterator)\n\u001b[1;32m   1202\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mStopIteration\u001b[39;00m:\n\u001b[1;32m   1203\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_iterator \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/All_Repos_Env/ENV_SPARK/lib/python3.10/site-packages/kafka/consumer/group.py:1116\u001b[0m, in \u001b[0;36mKafkaConsumer._message_generator_v2\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1114\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m_message_generator_v2\u001b[39m(\u001b[39mself\u001b[39m):\n\u001b[1;32m   1115\u001b[0m     timeout_ms \u001b[39m=\u001b[39m \u001b[39m1000\u001b[39m \u001b[39m*\u001b[39m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_consumer_timeout \u001b[39m-\u001b[39m time\u001b[39m.\u001b[39mtime())\n\u001b[0;32m-> 1116\u001b[0m     record_map \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mpoll(timeout_ms\u001b[39m=\u001b[39;49mtimeout_ms, update_offsets\u001b[39m=\u001b[39;49m\u001b[39mFalse\u001b[39;49;00m)\n\u001b[1;32m   1117\u001b[0m     \u001b[39mfor\u001b[39;00m tp, records \u001b[39min\u001b[39;00m six\u001b[39m.\u001b[39miteritems(record_map):\n\u001b[1;32m   1118\u001b[0m         \u001b[39m# Generators are stateful, and it is possible that the tp / records\u001b[39;00m\n\u001b[1;32m   1119\u001b[0m         \u001b[39m# here may become stale during iteration -- i.e., we seek to a\u001b[39;00m\n\u001b[1;32m   1120\u001b[0m         \u001b[39m# different offset, pause consumption, or lose assignment.\u001b[39;00m\n\u001b[1;32m   1121\u001b[0m         \u001b[39mfor\u001b[39;00m record \u001b[39min\u001b[39;00m records:\n\u001b[1;32m   1122\u001b[0m             \u001b[39m# is_fetchable(tp) should handle assignment changes and offset\u001b[39;00m\n\u001b[1;32m   1123\u001b[0m             \u001b[39m# resets; for all other changes (e.g., seeks) we'll rely on the\u001b[39;00m\n\u001b[1;32m   1124\u001b[0m             \u001b[39m# outer function destroying the existing iterator/generator\u001b[39;00m\n\u001b[1;32m   1125\u001b[0m             \u001b[39m# via self._iterator = None\u001b[39;00m\n",
      "File \u001b[0;32m~/All_Repos_Env/ENV_SPARK/lib/python3.10/site-packages/kafka/consumer/group.py:655\u001b[0m, in \u001b[0;36mKafkaConsumer.poll\u001b[0;34m(self, timeout_ms, max_records, update_offsets)\u001b[0m\n\u001b[1;32m    653\u001b[0m remaining \u001b[39m=\u001b[39m timeout_ms\n\u001b[1;32m    654\u001b[0m \u001b[39mwhile\u001b[39;00m \u001b[39mTrue\u001b[39;00m:\n\u001b[0;32m--> 655\u001b[0m     records \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_poll_once(remaining, max_records, update_offsets\u001b[39m=\u001b[39;49mupdate_offsets)\n\u001b[1;32m    656\u001b[0m     \u001b[39mif\u001b[39;00m records:\n\u001b[1;32m    657\u001b[0m         \u001b[39mreturn\u001b[39;00m records\n",
      "File \u001b[0;32m~/All_Repos_Env/ENV_SPARK/lib/python3.10/site-packages/kafka/consumer/group.py:702\u001b[0m, in \u001b[0;36mKafkaConsumer._poll_once\u001b[0;34m(self, timeout_ms, max_records, update_offsets)\u001b[0m\n\u001b[1;32m    699\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_client\u001b[39m.\u001b[39mpoll(timeout_ms\u001b[39m=\u001b[39m\u001b[39m0\u001b[39m)\n\u001b[1;32m    701\u001b[0m timeout_ms \u001b[39m=\u001b[39m \u001b[39mmin\u001b[39m(timeout_ms, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_coordinator\u001b[39m.\u001b[39mtime_to_next_poll() \u001b[39m*\u001b[39m \u001b[39m1000\u001b[39m)\n\u001b[0;32m--> 702\u001b[0m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_client\u001b[39m.\u001b[39;49mpoll(timeout_ms\u001b[39m=\u001b[39;49mtimeout_ms)\n\u001b[1;32m    703\u001b[0m \u001b[39m# after the long poll, we should check whether the group needs to rebalance\u001b[39;00m\n\u001b[1;32m    704\u001b[0m \u001b[39m# prior to returning data so that the group can stabilize faster\u001b[39;00m\n\u001b[1;32m    705\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_coordinator\u001b[39m.\u001b[39mneed_rejoin():\n",
      "File \u001b[0;32m~/All_Repos_Env/ENV_SPARK/lib/python3.10/site-packages/kafka/client_async.py:602\u001b[0m, in \u001b[0;36mKafkaClient.poll\u001b[0;34m(self, timeout_ms, future)\u001b[0m\n\u001b[1;32m    599\u001b[0m             timeout \u001b[39m=\u001b[39m \u001b[39mmin\u001b[39m(timeout, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mconfig[\u001b[39m'\u001b[39m\u001b[39mretry_backoff_ms\u001b[39m\u001b[39m'\u001b[39m])\n\u001b[1;32m    600\u001b[0m         timeout \u001b[39m=\u001b[39m \u001b[39mmax\u001b[39m(\u001b[39m0\u001b[39m, timeout)  \u001b[39m# avoid negative timeouts\u001b[39;00m\n\u001b[0;32m--> 602\u001b[0m     \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_poll(timeout \u001b[39m/\u001b[39;49m \u001b[39m1000\u001b[39;49m)\n\u001b[1;32m    604\u001b[0m \u001b[39m# called without the lock to avoid deadlock potential\u001b[39;00m\n\u001b[1;32m    605\u001b[0m \u001b[39m# if handlers need to acquire locks\u001b[39;00m\n\u001b[1;32m    606\u001b[0m responses\u001b[39m.\u001b[39mextend(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_fire_pending_completed_requests())\n",
      "File \u001b[0;32m~/All_Repos_Env/ENV_SPARK/lib/python3.10/site-packages/kafka/client_async.py:634\u001b[0m, in \u001b[0;36mKafkaClient._poll\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    631\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_register_send_sockets()\n\u001b[1;32m    633\u001b[0m start_select \u001b[39m=\u001b[39m time\u001b[39m.\u001b[39mtime()\n\u001b[0;32m--> 634\u001b[0m ready \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_selector\u001b[39m.\u001b[39;49mselect(timeout)\n\u001b[1;32m    635\u001b[0m end_select \u001b[39m=\u001b[39m time\u001b[39m.\u001b[39mtime()\n\u001b[1;32m    636\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_sensors:\n",
      "File \u001b[0;32m/usr/lib/python3.10/selectors.py:469\u001b[0m, in \u001b[0;36mEpollSelector.select\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    467\u001b[0m ready \u001b[39m=\u001b[39m []\n\u001b[1;32m    468\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m--> 469\u001b[0m     fd_event_list \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_selector\u001b[39m.\u001b[39;49mpoll(timeout, max_ev)\n\u001b[1;32m    470\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mInterruptedError\u001b[39;00m:\n\u001b[1;32m    471\u001b[0m     \u001b[39mreturn\u001b[39;00m ready\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# consume the data from the kafka broker and save to json file\n",
    "for message in Consumer:\n",
    "    message = message.value\n",
    "    with open('weather.json', 'a') as f:\n",
    "        json.dump(message, f)\n",
    "        f.write(' ') # add space between each json object\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "# close consumer to free up resources after reading the messages\n",
    "Consumer.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'_confluent-command',\n",
       " '_confluent-telemetry-metrics',\n",
       " '_confluent_balancer_api_state',\n",
       " '_confluent_balancer_broker_samples',\n",
       " '_confluent_balancer_partition_samples',\n",
       " '_schemas',\n",
       " 'connect-configs',\n",
       " 'connect-offsets',\n",
       " 'connect-status',\n",
       " 'weather'}"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Consumer.topics()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Created topic weather.\n"
     ]
    }
   ],
   "source": [
    "# create topic\n",
    "! kafka-topics --bootstrap-server localhost:9092 --create --topic weather --replication-factor 1 --partitions 1\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# delete topic\n",
    "! kafka-topics --bootstrap-server localhost:9092 --delete --topic weather"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Spark magic"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Unpacking and Exploding the  columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 187,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "weather_df = spark.read.json('weather.json')\n",
    "\n",
    "nested_json_type=type (weather_df.select(\"city\").take(1)[0][0]) # get the type of the nested json\n",
    "\n",
    "# function to expand the nested json object only\n",
    "def expand_json(df):\n",
    "    for c in df.columns:\n",
    "        if type(df.select(c).take(1)[0][0]) == nested_json_type:\n",
    "            df = df.select(\"*\", F.col(c+\".*\")).drop(c)\n",
    "            print(c)\n",
    "    return df\n",
    "    \n",
    "# weather_df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 188,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+---+---+-------+--------------------+\n",
      "|                city|cnt|cod|message|  every_3_hr_weather|\n",
      "+--------------------+---+---+-------+--------------------+\n",
      "|{{27.7167, 85.366...| 40|200|      0|{{20}, 1668600000...|\n",
      "|{{27.7167, 85.366...| 40|200|      0|{{24}, 1668610800...|\n",
      "|{{27.7167, 85.366...| 40|200|      0|{{26}, 1668621600...|\n",
      "|{{27.7167, 85.366...| 40|200|      0|{{2}, 1668632400,...|\n",
      "|{{27.7167, 85.366...| 40|200|      0|{{2}, 1668643200,...|\n",
      "+--------------------+---+---+-------+--------------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "\n",
    "#explode \"list\" column  as it is in list  of dictionary conataing 40 records\n",
    "weather_df = weather_df.select (\"*\", F.explode(\"list\").alias(\"every_3_hr_weather\")).drop(\"list\")\n",
    "\n",
    "weather_df.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 189,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "city\n",
      "every_3_hr_weather\n",
      "+---+---+-------+------------------+-------+-------+--------------+----------+----------+----------+--------+------+----------+-------------------+--------------------+----+------+---+----------+--------------------+-----------------+\n",
      "|cnt|cod|message|             coord|country|     id|          name|population|   sunrise|    sunset|timezone|clouds|        dt|             dt_txt|                main| pop|  rain|sys|visibility|             weather|             wind|\n",
      "+---+---+-------+------------------+-------+-------+--------------+----------+----------+----------+--------+------+----------+-------------------+--------------------+----+------+---+----------+--------------------+-----------------+\n",
      "| 40|200|      0|{27.7167, 85.3667}|     NP|1283647|Baudhatinchule|         0|1668559213|1668597990|   20700|  {20}|1668600000|2022-11-16 12:00:00|{290.89, 867, 68,...|0.29|{0.23}|{n}|     10000|[{light rain, 10n...|{268, 1.74, 1.58}|\n",
      "| 40|200|      0|{27.7167, 85.3667}|     NP|1283647|Baudhatinchule|         0|1668559213|1668597990|   20700|  {24}|1668610800|2022-11-16 15:00:00|{288.79, 868, 74,...| 0.0|  null|{n}|     10000|[{few clouds, 02n...|{311, 0.87, 0.67}|\n",
      "| 40|200|      0|{27.7167, 85.3667}|     NP|1283647|Baudhatinchule|         0|1668559213|1668597990|   20700|  {26}|1668621600|2022-11-16 18:00:00|{286.06, 868, 79,...| 0.0|  null|{n}|     10000|[{scattered cloud...| {74, 0.88, 0.51}|\n",
      "| 40|200|      0|{27.7167, 85.3667}|     NP|1283647|Baudhatinchule|         0|1668559213|1668597990|   20700|   {2}|1668632400|2022-11-16 21:00:00|{282.73, 866, 85,...| 0.0|  null|{n}|     10000|[{clear sky, 01n,...|{114, 1.04, 0.49}|\n",
      "| 40|200|      0|{27.7167, 85.3667}|     NP|1283647|Baudhatinchule|         0|1668559213|1668597990|   20700|   {2}|1668643200|2022-11-17 00:00:00|{283.02, 867, 85,...| 0.0|  null|{n}|     10000|[{clear sky, 01n,...| {90, 1.15, 0.74}|\n",
      "+---+---+-------+------------------+-------+-------+--------------+----------+----------+----------+--------+------+----------+-------------------+--------------------+----+------+---+----------+--------------------+-----------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# LEVEL 1 Unpacking (city and every_3_hr_weather  column)\n",
    "weather_df=expand_json(weather_df)\n",
    "weather_df.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 190,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "coord\n",
      "clouds\n",
      "main\n",
      "rain\n",
      "sys\n",
      "wind\n",
      "weather_dict\n",
      "+---+---+-------+-------+-------+--------------+----------+----------+----------+--------+----------+-------------------+----+----------+-------+-------+----------+----------+----------+--------+--------+---------+------+-------+--------+--------+----+---+---+----+-----+----------------+----+---+------+\n",
      "|cnt|cod|message|country|     id|          name|population|   sunrise|    sunset|timezone|        dt|             dt_txt| pop|visibility|    lat|    lon|cloudiness|feels_like|grnd_level|humidity|pressure|sea_level|  temp|temp_kf|temp_max|temp_min|  3h|pod|deg|gust|speed|     description|icon| id|  main|\n",
      "+---+---+-------+-------+-------+--------------+----------+----------+----------+--------+----------+-------------------+----+----------+-------+-------+----------+----------+----------+--------+--------+---------+------+-------+--------+--------+----+---+---+----+-----+----------------+----+---+------+\n",
      "| 40|200|      0|     NP|1283647|Baudhatinchule|         0|1668559213|1668597990|   20700|1668600000|2022-11-16 12:00:00|0.29|     10000|27.7167|85.3667|        20|    290.89|       867|      68|    1016|     1016|291.25|   4.61|  291.25|  286.64|0.23|  n|268|1.74| 1.58|      light rain| 10n|500|  Rain|\n",
      "| 40|200|      0|     NP|1283647|Baudhatinchule|         0|1668559213|1668597990|   20700|1668610800|2022-11-16 15:00:00| 0.0|     10000|27.7167|85.3667|        24|    288.79|       868|      74|    1016|     1016| 289.2|    4.1|   289.2|   285.1|null|  n|311|0.87| 0.67|      few clouds| 02n|801|Clouds|\n",
      "| 40|200|      0|     NP|1283647|Baudhatinchule|         0|1668559213|1668597990|   20700|1668621600|2022-11-16 18:00:00| 0.0|     10000|27.7167|85.3667|        26|    286.06|       868|      79|    1015|     1015| 286.6|   2.33|   286.6|  284.27|null|  n| 74|0.88| 0.51|scattered clouds| 03n|802|Clouds|\n",
      "| 40|200|      0|     NP|1283647|Baudhatinchule|         0|1668559213|1668597990|   20700|1668632400|2022-11-16 21:00:00| 0.0|     10000|27.7167|85.3667|         2|    282.73|       866|      85|    1013|     1013|283.43|    0.0|  283.43|  283.43|null|  n|114|1.04| 0.49|       clear sky| 01n|800| Clear|\n",
      "| 40|200|      0|     NP|1283647|Baudhatinchule|         0|1668559213|1668597990|   20700|1668643200|2022-11-17 00:00:00| 0.0|     10000|27.7167|85.3667|         2|    283.02|       867|      85|    1014|     1014|283.02|    0.0|  283.02|  283.02|null|  n| 90|1.15| 0.74|       clear sky| 01n|800| Clear|\n",
      "+---+---+-------+-------+-------+--------------+----------+----------+----------+--------+----------+-------------------+----+----------+-------+-------+----------+----------+----------+--------+--------+---------+------+-------+--------+--------+----+---+---+----+-----+----------------+----+---+------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#explode weather columnn ,it is in list  of dictionary conataing 1 record only\n",
    "weather_df = weather_df.select (\"*\", F.explode(\"weather\").alias(\"weather_dict\")).drop(\"weather\")\n",
    "\n",
    "# LEVEL 2 Unpacking  (coord,clouds,main,rain,sys,wind,weather_dict column)\n",
    "weather_df=expand_json(weather_df)\n",
    "\n",
    "\n",
    "# rename column all to coludiness\n",
    "weather_df = weather_df.withColumnRenamed(\"all\", \"cloudiness\")\n",
    "weather_df.show(5)\n",
    "\n",
    "# weather_df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 191,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- name: string (nullable = true)\n",
      " |-- sunrise: long (nullable = true)\n",
      " |-- sunset: long (nullable = true)\n",
      " |-- dt: long (nullable = true)\n",
      " |-- dt_txt: string (nullable = true)\n",
      " |-- pop: double (nullable = true)\n",
      " |-- visibility: long (nullable = true)\n",
      " |-- lat: double (nullable = true)\n",
      " |-- lon: double (nullable = true)\n",
      " |-- cloudiness: long (nullable = true)\n",
      " |-- feels_like: double (nullable = true)\n",
      " |-- grnd_level: long (nullable = true)\n",
      " |-- humidity: long (nullable = true)\n",
      " |-- sea_level: long (nullable = true)\n",
      " |-- temp: double (nullable = true)\n",
      " |-- temp_kf: double (nullable = true)\n",
      " |-- temp_max: double (nullable = true)\n",
      " |-- temp_min: double (nullable = true)\n",
      " |-- pod: string (nullable = true)\n",
      " |-- deg: long (nullable = true)\n",
      " |-- gust: double (nullable = true)\n",
      " |-- speed: double (nullable = true)\n",
      " |-- description: string (nullable = true)\n",
      " |-- main: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#  delete the column which is not required\n",
    "weather_df = weather_df.drop(\"cod\",\"message\",\"cnt\",\"pressure\",\"3h\",\"icon\",\"timezone\",\"population\",\"country\",\"id\")\n",
    "\n",
    "weather_df.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Important functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 192,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------+-------------------+-------------------+-------------------+-------------------+----+----------+-------+-------+----------+----------+----------+--------+---------+------+-------+--------+--------+---+---+----+-----+----------------+------+\n",
      "|          name|            sunrise|             sunset|                 dt|             dt_txt| pop|visibility|    lat|    lon|cloudiness|feels_like|grnd_level|humidity|sea_level|  temp|temp_kf|temp_max|temp_min|pod|deg|gust|speed|     description|  main|\n",
      "+--------------+-------------------+-------------------+-------------------+-------------------+----+----------+-------+-------+----------+----------+----------+--------+---------+------+-------+--------+--------+---+---+----+-----+----------------+------+\n",
      "|Baudhatinchule|2022-11-16 06:25:13|2022-11-16 17:11:30|2022-11-16 17:45:00|2022-11-16 12:00:00|0.29|     10000|27.7167|85.3667|        20|    290.89|       867|      68|     1016|291.25|   4.61|  291.25|  286.64|  n|268|1.74| 1.58|      light rain|  Rain|\n",
      "|Baudhatinchule|2022-11-16 06:25:13|2022-11-16 17:11:30|2022-11-16 20:45:00|2022-11-16 15:00:00| 0.0|     10000|27.7167|85.3667|        24|    288.79|       868|      74|     1016| 289.2|    4.1|   289.2|   285.1|  n|311|0.87| 0.67|      few clouds|Clouds|\n",
      "|Baudhatinchule|2022-11-16 06:25:13|2022-11-16 17:11:30|2022-11-16 23:45:00|2022-11-16 18:00:00| 0.0|     10000|27.7167|85.3667|        26|    286.06|       868|      79|     1015| 286.6|   2.33|   286.6|  284.27|  n| 74|0.88| 0.51|scattered clouds|Clouds|\n",
      "|Baudhatinchule|2022-11-16 06:25:13|2022-11-16 17:11:30|2022-11-17 02:45:00|2022-11-16 21:00:00| 0.0|     10000|27.7167|85.3667|         2|    282.73|       866|      85|     1013|283.43|    0.0|  283.43|  283.43|  n|114|1.04| 0.49|       clear sky| Clear|\n",
      "|Baudhatinchule|2022-11-16 06:25:13|2022-11-16 17:11:30|2022-11-17 05:45:00|2022-11-17 00:00:00| 0.0|     10000|27.7167|85.3667|         2|    283.02|       867|      85|     1014|283.02|    0.0|  283.02|  283.02|  n| 90|1.15| 0.74|       clear sky| Clear|\n",
      "+--------------+-------------------+-------------------+-------------------+-------------------+----+----------+-------+-------+----------+----------+----------+--------+---------+------+-------+--------+--------+---+---+----+-----+----------------+------+\n",
      "only showing top 5 rows\n",
      "\n",
      "root\n",
      " |-- name: string (nullable = true)\n",
      " |-- sunrise: string (nullable = true)\n",
      " |-- sunset: string (nullable = true)\n",
      " |-- dt: string (nullable = true)\n",
      " |-- dt_txt: string (nullable = true)\n",
      " |-- pop: double (nullable = true)\n",
      " |-- visibility: long (nullable = true)\n",
      " |-- lat: double (nullable = true)\n",
      " |-- lon: double (nullable = true)\n",
      " |-- cloudiness: long (nullable = true)\n",
      " |-- feels_like: double (nullable = true)\n",
      " |-- grnd_level: long (nullable = true)\n",
      " |-- humidity: long (nullable = true)\n",
      " |-- sea_level: long (nullable = true)\n",
      " |-- temp: double (nullable = true)\n",
      " |-- temp_kf: double (nullable = true)\n",
      " |-- temp_max: double (nullable = true)\n",
      " |-- temp_min: double (nullable = true)\n",
      " |-- pod: string (nullable = true)\n",
      " |-- deg: long (nullable = true)\n",
      " |-- gust: double (nullable = true)\n",
      " |-- speed: double (nullable = true)\n",
      " |-- description: string (nullable = true)\n",
      " |-- main: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# function to convert the unix timestamp to date and time \n",
    "def convert_unix_to_date_time(df):\n",
    "    for c in df.columns:\n",
    "        if c == \"dt\":\n",
    "            df = df.withColumn(c, F.from_unixtime(F.col(c)))\n",
    "        elif c == \"sunrise\":\n",
    "            df = df.withColumn(c, F.from_unixtime(F.col(c)))\n",
    "        elif c == \"sunset\":\n",
    "            df = df.withColumn(c, F.from_unixtime(F.col(c)))\n",
    "    return df\n",
    "\n",
    "# convert the unix timestamp to date and time\n",
    "weather_df = convert_unix_to_date_time(weather_df)\n",
    "\n",
    "weather_df.show(5)\n",
    "\n",
    "weather_df.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 194,
   "metadata": {},
   "outputs": [],
   "source": [
    "Producer_save_table = KafkaProducer(bootstrap_servers=[\n",
    "                         'localhost:9092'], value_serializer=lambda x: dumps(x).encode('utf-8'))\n",
    "\n",
    "for row in weather_df.rdd.collect():\n",
    "    Producer_save_table.send('cleaned_table', value=row.asDict())\n",
    "    Producer_save_table.flush()  # flush the data to the kafka broker ( topic) and  make sure data  is sent to the kafka broker and  not lost in the buffer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Caused by: org.apache.kafka.connect.errors.DataException: JsonConverter with schemas.enable requires \"schema\" and \"payload\" fields and may not contain additional fields. If you are trying to deserialize plain JSON data, set schemas.enable=false in your converter configuration.\n",
    "\n",
    "{\n",
    "    \"name\": \"cleaned_table-sink-connector\",\n",
    "    \"config\": {\n",
    "        \"connector.class\": \"io.confluent.connect.jdbc.JdbcSinkConnector\",\n",
    "        \"connection.url\": \"jdbc:postgresql://localhost:5432/sink\",\n",
    "        \"connection.user\": \"postgres\",\n",
    "        \"connection.password\":\"1234\",\n",
    "        \"auto.create\": \"true\",\n",
    "        \"auto.evolve\": \"true\",\n",
    "        \"table.name.format\": \"cleaned\",\n",
    "        \"tasks.max\": \"1\",\n",
    "        \"topics\": \"cleaned_table\",\n",
    "        \"insert.mode\": \"upsert\",\n",
    "        \"schema.ignore\": \"true\"\n",
    "        # <!-- #set schemas.enable=false in your converter configuration. -->\n",
    "        \"key.converter.schemas.enable\": \"false\",\n",
    "\n",
    "        \n",
    "\n",
    "    }\n",
    "\n",
    "}\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "lkl\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.6 ('ENV_SPARK': venv)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "2214088c3d0387d9d6dcbb7c6a9ab667498cf089874e7aaa07c32225f40a1e68"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
